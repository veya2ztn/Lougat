{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d6134e-87c9-4170-a817-b9c7c3accde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/locr/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/zhangtianning/projects/PromptNougat\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08487cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170177c3-6417-4132-912b-b03c3ad12ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_arguements import FlougatSmallConfig,UpArouGatSmallConfig\n",
    "from model.model_arguements import TrainerModelConfig\n",
    "from model.trainer_model import PromptNougatModel, LlougatTrainerModel\n",
    "from model.replace_flash_attn import replace_promptdecoder_attn_with_flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0421d9-014b-4e86-bd9f-ac6120981750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from simple_parsing import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_arguments(UpArouGatSmallConfig, dest=\"config\")\n",
    "config = parser.parse_args([\"--attn_implementation\",\"sdpa\",\n",
    "                            \"--processor_fold\",\"config/processor/uparxive2k\"]).config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2915a9-c3b7-4e30-a5ec-a887e59a313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "config.encoder.input_size    = [2048,1472]\n",
    "config.encoder.patch_size    = [7,7,3,3]\n",
    "config.encoder.patch_stride  = [4,4,2,2]\n",
    "config.encoder.patch_padding = [3,3,1,1]\n",
    "config.encoder.image_embedding_size  = [32, 23]\n",
    "huggingface_config = config.huggingface_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7cb787-f8dc-4ff7-9041-4f65880670a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 23]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_config.encoder.image_embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63bf0bad-59d5-4310-86e0-650988822e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 23]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "encoder = AutoModel.from_config(huggingface_config.encoder)\n",
    "_input = torch.randn(1,3,2048,1472).cuda()\n",
    "encoder = encoder.cuda()\n",
    "print(encoder.config.image_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ded8463-9cba-4546-a07a-79429a02e459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 736, 1024])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    print(encoder.vision_tower.forward_features_unpool(_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "055bfb17-6638-495b-a51b-e6f63c140106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 736, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    print(encoder(_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3f20d7-00ae-4f40-812d-99fb5ca5d9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model =  LlougatTrainerModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f68d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7103374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.load(\"input.pt\",map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9027487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12582912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4* 4096* 16* 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e38286a-b50a-40cd-a50a-977b632ef303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "old_weight = torch.load(\"pretrain_weight/models--microsoft--Florence-2-base-ft/snapshots/e7a5acc73559546de6e12ec0319cd7cc1fa2437c/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dcd498-89d3-4bc6-b9bf-58b5abe94a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_keys = set(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c11f71b-284e-47c5-8d6b-4f5dcbe7f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d730b112-d0da-4057-987d-d6ed1e4029fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in old_weight.keys():\n",
    "    vision_key = \"encoder.\"+key\n",
    "    if vision_key in state_dict:\n",
    "        state_dict[vision_key] = old_weight[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87eff451-d3e1-44ce-ac38-4ca762f1b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dict, \"pretrain_weight/FlougatSmall_start.model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f142e901-e62f-4a4d-af11-fdc92993983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlougatForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlougatModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    }
   ],
   "source": [
    "from decoder.llougat.modeling_llougat import *\n",
    "hidden_size = 32\n",
    "config = LlougatConfig(image_size = [896,672], image_embedding_size=[28, 21],num_hidden_layers=2,\n",
    "                       num_attention_heads=16,num_key_value_heads=16,intermediate_size=1024,\n",
    "                       hidden_size=hidden_size, _attn_implementation='flash_attention_2')\n",
    "decoder = LlougatForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f6c353e-fa64-42b5-a149-1005de385136",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder.cuda()\n",
    "encoder_hidden_states  = torch.randn(2, 28*21, hidden_size)\n",
    "input_ids    = torch.randint(0,32,(2, 10))\n",
    "input_bboxes = torch.randn((2, 10, 4))\n",
    "input_ids[0][5:]=0\n",
    "input_bboxes[0][5:]*=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1227d342-4a3f-4226-b6f3-1943e1d91149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/ztn/projects/PromptNougat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tntorch/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d646cd99-8b92-403b-8798-f8c131307964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch = torch.load('input.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7f6c86-5e84-4b5c-9efa-9c926f5ae72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0, 42,  ...,  1,  1,  1],\n",
       "        [ 0,  0, 69,  ...,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['pre_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4ecc57-583c-4051-b335-4b6f790e0a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1728, 0.1565],\n",
       "         [0.2193, 0.1707]],\n",
       "\n",
       "        [[0.1728, 0.1565],\n",
       "         [0.2193, 0.1707]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['prompt_in'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b07d4-090f-4169-8475-444d2f06e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fd203-654f-4c45-acc6-11346e432969",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Rlougat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773b6834-1d51-4b44-abe3-47315ed9d2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/zhangtianning/projects/PromptNougat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d449a59c-21a9-439c-956e-0c702beb3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.decoder.Rlougat.modeling_Rlougat import *\n",
    "head_num = 8\n",
    "num_attention_heads=128\n",
    "config = RlougatConfig(image_size = [896,672], image_embedding_size=[28, 21],num_hidden_layers=2,\n",
    "                       num_attention_heads=head_num,num_key_value_heads=head_num,intermediate_size=1024,\n",
    "                       hidden_size = head_num*num_attention_heads, _attn_implementation='eager',coordinate_retreive_method=\"position_revert\")\n",
    "hidden_size = config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac89d9d5-fcb7-45ff-a7f6-09edb754b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RlougatForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83213a58-7c5f-4821-a132-9bd8df130d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000],\n",
       "          [0.0476, 0.0357]],\n",
       "\n",
       "         [[0.0500, 0.0000],\n",
       "          [0.0952, 0.0357]],\n",
       "\n",
       "         [[0.1000, 0.0000],\n",
       "          [0.1429, 0.0357]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.9000, 1.0000],\n",
       "          [0.9048, 1.0000]],\n",
       "\n",
       "         [[0.9500, 1.0000],\n",
       "          [0.9524, 1.0000]],\n",
       "\n",
       "         [[1.0000, 1.0000],\n",
       "          [1.0000, 1.0000]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.model.img_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d04faf2-5552-4453-b778-2a9fd7efc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder.cuda()\n",
    "encoder_hidden_states  = torch.randn(2, 28*21, hidden_size)\n",
    "input_ids    = torch.randint(0,hidden_size,(2, 10))\n",
    "input_bboxes = torch.rand((2, 10, 4))\n",
    "input_ids[0][5:]=0\n",
    "input_bboxes[0][5:]*=0\n",
    "\n",
    "self = decoder\n",
    "hidden_states = inputs_embeds  = decoder.model.embed_tokens(input_ids.cuda()).cuda()\n",
    "encoder_hidden_states = torch.randn(2, 28*21, hidden_size).cuda()\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "attention_mask[0][5:]=0\n",
    "attention_mask = attention_mask.cuda()\n",
    "B, S, _ = hidden_states.shape\n",
    "B, L, _ = encoder_hidden_states.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d286b45f-c777-4056-9730-acd6e4eae644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "input_bboxes_states = input_bboxes\n",
    "coordinate_encoding = self.model.img_coord  # [1,588,2,2]->[1,588,2,d]\n",
    "outputs = self.model(\n",
    "    input_ids             = None,\n",
    "    inputs_embeds         = inputs_embeds,\n",
    "    encoder_hidden_states = encoder_hidden_states,\n",
    "    input_bboxes          = input_bboxes,\n",
    "    input_bboxes_states   = input_bboxes,\n",
    "    coordinate_encoding   = coordinate_encoding,\n",
    "    attention_mask        = attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25210ec3-3db7-4671-856a-139cf98f85f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-1.1260,  0.1510, -1.3493,  ...,  0.7592,  0.1440,  1.0500],\n",
       "         [-0.8243,  0.0225, -1.2280,  ...,  0.8697,  0.3813,  1.1116],\n",
       "         [-0.8488, -0.0627, -1.1820,  ...,  0.3753,  0.4269,  0.5371],\n",
       "         ...,\n",
       "         [-0.7363,  0.0472, -1.1683,  ...,  0.3056,  0.3020,  1.2315],\n",
       "         [-0.7641,  0.0704, -1.1754,  ...,  0.3322,  0.2351,  1.2248],\n",
       "         [-0.7950,  0.0727, -1.1913,  ...,  0.3579,  0.1915,  1.2232]],\n",
       "\n",
       "        [[-0.1681, -0.9308, -0.3830,  ...,  0.4013, -1.6478, -0.0235],\n",
       "         [-0.0679, -0.6722, -0.5089,  ...,  0.1562, -1.6467,  0.3397],\n",
       "         [-0.5769, -1.1116, -0.0852,  ...,  0.2720, -1.8343,  0.3134],\n",
       "         ...,\n",
       "         [-0.0831, -0.6901,  0.0504,  ..., -0.5693, -2.2333,  0.0663],\n",
       "         [ 0.1603, -1.2100, -0.1210,  ...,  0.4392, -1.9478, -0.0898],\n",
       "         [ 0.0398, -0.3920, -0.5588,  ...,  0.4499, -1.8137, -0.2331]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>), past_key_values=DynamicCache(), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438f4a00-7b0d-4174-8e99-5241d9964736",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = self.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1daaa5-9e93-452c-9832-d18343a2a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_position = position_ids = past_key_values  = None\n",
    "output_attentions = False\n",
    "if cache_position is None:\n",
    "    past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "    cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)\n",
    "if position_ids is None:\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "causal_mask     = self.model._update_causal_mask(attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions)\n",
    "cross_attn_mask = self.model.get_cross_attn_mask(attention_mask, inputs_embeds, encoder_hidden_states)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0e511-e85b-419a-9621-d3f3fbc29af8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1176x32 and 256x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m causal_mask     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_update_causal_mask(attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions)\n\u001b[1;32m      9\u001b[0m cross_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_cross_attn_mask(attention_mask, inputs_embeds, encoder_hidden_states)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_bboxes_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_bboxes_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinate_encoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcoordinate_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/zhangtianning/projects/PromptNougat/model/decoder/Rlougat/modeling_Rlougat.py:563\u001b[0m, in \u001b[0;36mRlougatDecoderLayer.forward\u001b[0;34m(self, hidden_states, input_bboxes_states, attention_mask, cross_attn_mask, encoder_hidden_states, coordinate_encoding, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    561\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    562\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_layer_norm(hidden_states)\n\u001b[0;32m--> 563\u001b[0m hidden_states, cross_attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# [bs,len,1024]              # <- q can be [bs,1,1024] or [bs, len ,1024]\u001b[39;49;00m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_q\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_bboxes_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# None\u001b[39;49;00m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states_kv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# k,v: image, [bs,588,1024]  # <-kv which is fixed as [bs,588,1024]\u001b[39;49;00m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_kv\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcoordinate_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# None\u001b[39;49;00m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcross_attn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# None\u001b[39;49;00m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# None\u001b[39;49;00m\n\u001b[1;32m    570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    573\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/zhangtianning/projects/PromptNougat/model/decoder/Rlougat/modeling_Rlougat.py:228\u001b[0m, in \u001b[0;36mRlougatCrossAttention.forward\u001b[0;34m(self, hidden_states, position_q, hidden_states_kv, position_kv, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    220\u001b[0m     hidden_states   : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[0;32m--> 228\u001b[0m     query_states, key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_qkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhidden_states_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_kv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     bsz, H, q_len, d \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    230\u001b[0m     bsz, H, k_len, d \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/mnt/data/zhangtianning/projects/PromptNougat/model/decoder/Rlougat/modeling_Rlougat.py:203\u001b[0m, in \u001b[0;36mRlougatCrossAttention.generate_qkv\u001b[0;34m(self, hidden_states, position_q, hidden_states_kv, position_kv)\u001b[0m\n\u001b[1;32m    200\u001b[0m     hidden_states_k, hidden_states_v \u001b[38;5;241m=\u001b[39m hidden_states_kv\n\u001b[1;32m    202\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states_q)\n\u001b[0;32m--> 203\u001b[0m key_states   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states_v)\n\u001b[1;32m    205\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len,           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)   \u001b[38;5;66;03m# (B,H,L,D)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1176x32 and 256x256)"
     ]
    }
   ],
   "source": [
    "   \n",
    "decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    input_bboxes_states = input_bboxes_states, \n",
    "    attention_mask=causal_mask,\n",
    "    cross_attn_mask=cross_attn_mask,\n",
    "    encoder_hidden_states = encoder_hidden_states, \n",
    "    coordinate_encoding = coordinate_encoding, \n",
    "    position_ids = position_ids, \n",
    "    past_key_value = past_key_values, \n",
    "    output_attentions = output_attentions, \n",
    "    use_cache = False, \n",
    "    cache_position = cache_position, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e71042d-f328-44d5-a1ff-586a9ae4ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = decoder_layer\n",
    "past_key_value =None\n",
    "use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c51f34f7-52bf-42b4-b784-265546c318a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "701afd35-0091-4192-a5b6-5934d79be26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual                 = hidden_states\n",
    "hidden_states            = self.self_attn_layernorm(hidden_states)\n",
    "self_attn_past_key_value = past_key_value if past_key_value is not None else None\n",
    "\"\"\"\n",
    "(B, L, D) x (B, L, D) x (B, L, D) -> (B, L, D)\n",
    "\"\"\"\n",
    "hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "    hidden_states    = hidden_states,\n",
    "    attention_mask   = causal_mask,\n",
    "    position_ids     = position_ids,\n",
    "    past_key_value   = self_attn_past_key_value,\n",
    "    output_attentions= output_attentions,\n",
    "    use_cache        = use_cache,\n",
    "    cache_position   = cache_position,\n",
    ")\n",
    "hidden_states = residual + hidden_states\n",
    "residual = hidden_states\n",
    "hidden_states = self.cross_attn_layer_norm(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c2f954-fb74-493e-8c6c-2ebe28aa3892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_states = residual + hidden_states\n",
    "residual = hidden_states\n",
    "hidden_states = self.cross_attn_layer_norm(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e051f697-a644-4886-a701-dec47498e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bboxes_states = input_bboxes\n",
    "coordinate_encoding = decoder.model.img_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d791e2-2a44-43ef-a7dc-28fa43ed6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states    = hidden_states      \n",
    "position_q       = input_bboxes_states\n",
    "hidden_states_kv = encoder_hidden_states\n",
    "position_kv       = coordinate_encoding\n",
    "attention_mask   = cross_attn_mask     \n",
    "output_attentions= False              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb0af5c1-4ba7-46aa-92ad-d5a4e058d3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_states, key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_qkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhidden_states_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_kv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/zhangtianning/projects/PromptNougat/model/decoder/Rlougat/modeling_Rlougat.py:210\u001b[0m, in \u001b[0;36mRlougatCrossAttention.generate_qkv\u001b[0;34m(self, hidden_states, position_q, hidden_states_kv, position_kv)\u001b[0m\n\u001b[1;32m    206\u001b[0m key_states   \u001b[38;5;241m=\u001b[39m   key_states\u001b[38;5;241m.\u001b[39mview(bsz, k_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)   \u001b[38;5;66;03m# (B,H,L,D)\u001b[39;00m\n\u001b[1;32m    207\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, v_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads,            \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)   \u001b[38;5;66;03m# (B,H,L,D)\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m cos, sin     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m query_states \u001b[38;5;241m=\u001b[39m apply_position_pos_emb( query_states, cos, sin)\n\u001b[1;32m    212\u001b[0m cos, sin     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_emb(position_kv)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/zhangtianning/projects/PromptNougat/model/decoder/Rlougat/modeling_Rlougat.py:86\u001b[0m, in \u001b[0;36mPositionRotaryEmbedding.forward\u001b[0;34m(self, position_ids)\u001b[0m\n\u001b[1;32m     84\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(position_ids\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolution)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     85\u001b[0m cos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcos_table[position_ids]\n\u001b[0;32m---> 86\u001b[0m sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msin_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cos, sin\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "query_states, key_states, value_states = self.cross_attn.generate_qkv(hidden_states, position_q,hidden_states_kv, position_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58be7a-eaad-45b1-94be-3d65624e6646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1176x32 and 256x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m      3\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_layer_norm(hidden_states)\n\u001b[0;32m----> 4\u001b[0m hidden_states, cross_attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# [bs,len,1024]              # <- q can be [bs,1,1024] or [bs, len ,1024]\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_q\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_bboxes_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# None\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states_kv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# k,v: image, [bs,588,1024]  # <-kv which is fixed as [bs,588,1024]\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_kv\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcoordinate_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# None\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcross_attn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# None\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# None\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/zhangtianning/projects/PromptNougat/model/decoder/Rlougat/modeling_Rlougat.py:228\u001b[0m, in \u001b[0;36mRlougatCrossAttention.forward\u001b[0;34m(self, hidden_states, position_q, hidden_states_kv, position_kv, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    220\u001b[0m     hidden_states   : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[0;32m--> 228\u001b[0m     query_states, key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_qkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhidden_states_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_kv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     bsz, H, q_len, d \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    230\u001b[0m     bsz, H, k_len, d \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/mnt/data/zhangtianning/projects/PromptNougat/model/decoder/Rlougat/modeling_Rlougat.py:203\u001b[0m, in \u001b[0;36mRlougatCrossAttention.generate_qkv\u001b[0;34m(self, hidden_states, position_q, hidden_states_kv, position_kv)\u001b[0m\n\u001b[1;32m    200\u001b[0m     hidden_states_k, hidden_states_v \u001b[38;5;241m=\u001b[39m hidden_states_kv\n\u001b[1;32m    202\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states_q)\n\u001b[0;32m--> 203\u001b[0m key_states   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states_v)\n\u001b[1;32m    205\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len,           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)   \u001b[38;5;66;03m# (B,H,L,D)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1176x32 and 256x256)"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_states, cross_attn_weights, _ = forward(self.cross_attn,\n",
    "    hidden_states    = hidden_states,              # [bs,len,1024]              # <- q can be [bs,1,1024] or [bs, len ,1024]\n",
    "    position_q       = input_bboxes_states,        # None\n",
    "    hidden_states_kv = encoder_hidden_states,      # k,v: image, [bs,588,1024]  # <-kv which is fixed as [bs,588,1024]\n",
    "    position_kv       = coordinate_encoding,        # None\n",
    "    attention_mask   = cross_attn_mask,            # None\n",
    "    output_attentions= False,                      # None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9783efa9-4274-4755-8546-7431302361f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "        self,\n",
    "        hidden_states   : torch.Tensor,\n",
    "        position_q: Optional[torch.Tensor] = None,\n",
    "        hidden_states_kv: Optional[Union[torch.Tensor, Tuple[torch.Tensor,torch.Tensor]]] = None,\n",
    "        position_kv: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \n",
    "        query_states, key_states, value_states = self.generate_qkv(hidden_states, position_q,hidden_states_kv, position_kv)\n",
    "        bsz, H, q_len, d = query_states.shape\n",
    "        bsz, H, k_len, d = key_states.shape\n",
    "        bsz, H, v_len, d = value_states.shape\n",
    "        \n",
    "        if attention_mask is not None and attention_mask.shape[-2] > q_len:\n",
    "            if q_len == 1:\n",
    "                attention_mask = attention_mask[:, :, -q_len:]\n",
    "            else:\n",
    "                raise NotImplementedError(f\"the cross-attention_mask shape = {attention_mask.shape} which is not correct to the desired shape = {(1, 1, q_len,k_len)}\")\n",
    "        key_states   =    repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states =    repeat_kv(value_states, self.num_key_value_groups)\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) /math.sqrt(d) \n",
    "\n",
    "        if attention_mask is not None: \n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output  = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, d):\n",
    "            raise ValueError(f\"\"\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"f\" {attn_output.size()}. \n",
    "                             Debug: q.shape={query_states.shape} \n",
    "                                    k.shape={key_states.shape} \n",
    "                                    v.shape={value_states.shape} \n",
    "                                    attn_weights.shape={attn_weights.shape} \n",
    "                                    attention_mask.shape={attention_mask.shape}\n",
    "                             \"\"\")\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, d*H)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeba927-faa9-47d6-bb7c-a987b1322e20",
   "metadata": {},
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d155206a-351d-401d-8a48-07d73b24addf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use position decay == 1, please note it is not the best setting for generation\n"
     ]
    }
   ],
   "source": [
    "from model.decoder.llougat.modeling_llougat import *\n",
    "hidden_size = 32\n",
    "config = LlougatConfig(image_size = [896,672], image_embedding_size=[28, 21],num_hidden_layers=2,\n",
    "                       num_attention_heads=16,num_key_value_heads=16,intermediate_size=1024,\n",
    "                       hidden_size=hidden_size, _attn_implementation='sdpa',coordinate_retreive_method=\"heatmap_mapping\")\n",
    "decoder = LlougatForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25c03c0-5ec6-4edb-8a25-6001b117a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use position decay == 1, please note it is not the best setting for generation\n"
     ]
    }
   ],
   "source": [
    "from model.decoder.locr.modeling_locr import *\n",
    "hidden_size = 128\n",
    "config = PromptBartConfig(image_size = [896,672], image_embedding_size=[28, 21],num_hidden_layers=2,\n",
    "                         num_attention_heads=16,num_key_value_heads=16,intermediate_size=1024,\n",
    "                         hidden_size=hidden_size,BartAttention_implement='flashattn')\n",
    "decoder = PromptBartForCausalLM(config).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53312db1-a3ef-4345-9365-8de26373e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder.cuda()\n",
    "encoder_hidden_states  = torch.randn(2, 28*21, hidden_size)\n",
    "input_ids    = torch.randint(0,hidden_size,(2, 10))\n",
    "input_bboxes = torch.randn((2, 10, 4))\n",
    "input_ids[0][5:]=0\n",
    "input_bboxes[0][5:]*=0\n",
    "\n",
    "self = decoder\n",
    "hidden_states = inputs_embeds  = decoder.model.embed_tokens(input_ids.cuda()).cuda()\n",
    "encoder_hidden_states = torch.randn(2, 28*21, hidden_size).cuda()\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "attention_mask[0][5:]=0\n",
    "attention_mask = attention_mask.cuda()\n",
    "B, S, _ = hidden_states.shape\n",
    "B, L, _ = encoder_hidden_states.shape\n",
    "self = decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed3cdd1-52eb-4288-a561-55f09bf1d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_position = torch.arange(0, 0 + inputs_embeds.shape[1], device=inputs_embeds.device)\n",
    "position_ids = cache_position.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87df06ef-030b-4cd6-b0bc-a603ae3103b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.decoder.locr.modeling_locr import PromptAttentionFlash,PromptAttention,_prepare_4d_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91eec1b6-44df-4504-aa57-e96928de3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6fd358-4f4b-462c-94f5-fae5a1aee94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states = torch.randn(2, 28*21, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb872dc-68d3-4c65-b36e-cc39db91e222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlougatCrossSdpaAttention is using LlougatCrossSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    decoder(\n",
    "input_ids = input_ids.cuda(),\n",
    "input_bboxes = input_bboxes.cuda(),\n",
    "input_bboxes_states = None,\n",
    "encoder_hidden_states = encoder_hidden_states.cuda(),\n",
    "coordinate_encoding = None,\n",
    "attention_mask = attention_mask.cuda(),\n",
    "position_ids = position_ids,\n",
    "past_key_values = None,\n",
    "inputs_embeds = None,\n",
    "use_cache = False,\n",
    "output_attentions = False,\n",
    "output_hidden_states = False,\n",
    "return_dict = False,\n",
    "cache_position = cache_position,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "062975f4-a5ab-42f3-b6f8-ad333c4bc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        decoder.decoder.config.BartAttention_implement = 'flashattn'\n",
    "        causal_mask = decoder.decoder._prepare_decoder_attention_mask(\n",
    "            attention_mask, None, None, input_shape, inputs_embeds,0\n",
    "        )\n",
    "        result1= MBartFlashAttention2.forward(decoder.decoder.layers[0].self_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                attention_mask   = causal_mask.cuda(),   \n",
    "            )[0]\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        decoder.decoder.config.BartAttention_implement = 'eager'\n",
    "        causal_mask = decoder.decoder._prepare_decoder_attention_mask(\n",
    "            attention_mask, None, None, input_shape, inputs_embeds,0\n",
    "        )\n",
    "        result2= MBartAttention.forward(decoder.decoder.layers[0].self_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                attention_mask   = causal_mask.cuda(),   \n",
    "            )[0]\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        decoder.decoder.config.BartAttention_implement = 'sdpa'\n",
    "        causal_mask = decoder.decoder._prepare_decoder_attention_mask(\n",
    "            attention_mask, None, None, input_shape, inputs_embeds,0\n",
    "        )\n",
    "        result3= MBartAttentionSDPA.forward(decoder.decoder.layers[0].self_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                attention_mask   = causal_mask.cuda(),   \n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86ed0ac-a717-4dd3-8175-696c1c22f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = attention_mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f70c8c49-359c-4689-8b97-73d18bfe1ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0200, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(result1[attention_mask],result2[attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d2104f-959e-4ddc-9387-18764e393e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0200, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(result1[attention_mask],result3[attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233ea29b-5030-48ef-9518-c5a50d5fd209",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PromptBartForCausalLM' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m         cross_attn_mask \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n\u001b[1;32m      5\u001b[0m         result1\u001b[38;5;241m=\u001b[39m LlougatCrossAttention\u001b[38;5;241m.\u001b[39mforward(decoder\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcross_attn,\n\u001b[1;32m      6\u001b[0m                 hidden_states    \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mcuda(),              \n\u001b[1;32m      7\u001b[0m                 hidden_states_kv \u001b[38;5;241m=\u001b[39m encoder_hidden_states\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m      8\u001b[0m                 attention_mask   \u001b[38;5;241m=\u001b[39m cross_attn_mask\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      9\u001b[0m             )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PromptBartForCausalLM' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'eager'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result1= LlougatCrossAttention.forward(decoder.model.layers[0].cross_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv = encoder_hidden_states.cuda(),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'flash_attention_2'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result2= LlougatCrossFlashAttention2.forward(decoder.model.layers[0].cross_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv = encoder_hidden_states.cuda(),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'sdpa'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result3= LlougatCrossSdpaAttention.forward(decoder.model.layers[0].cross_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv = encoder_hidden_states.cuda(),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8b14861-8bfc-4ffa-8507-3ee691f21584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(result1[attention_mask],result3[attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5564b3-aea5-4388-b607-0f2239acbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate = torch.randn(2, 28*21, 4).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8993f1be-51e7-40d8-9fbe-6eeba25440ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 s  315 ns per loop (mean  std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'eager'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result1= LlougatCrossAttention.forward(decoder.position_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv =(encoder_hidden_states.cuda(), coordinate),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da6ef92-959d-4802-8afd-a39d588d7a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 s  1.34 s per loop (mean  std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'sdpa'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result3= LlougatCrossSdpaAttention.forward(decoder.position_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv =(encoder_hidden_states.cuda(),coordinate ),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a4a8f2-cfaf-4adf-a25d-091ed71a7ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(result1[attention_mask],result3[attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895376e5-9da2-46b6-befa-5bc7e4253696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The default implementation runs in 666.268 microseconds\n",
      "The math implementation runs in 3759.861 microseconds\n",
      "The flash attention implementation runs in 665.455 microseconds\n",
      "The memory efficient implementation runs in 1816.581 microseconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Example Usage:\n",
    "query, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\n",
    "\n",
    "import torch.utils.benchmark as benchmark\n",
    "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6\n",
    "\n",
    "# Lets define the hyper-parameters of our input\n",
    "batch_size = 32\n",
    "max_sequence_len = 1024\n",
    "num_heads = 32\n",
    "embed_dimension = 32\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "key = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "value = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "\n",
    "print(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
    "\n",
    "# Lets explore the speed of each of the 3 implementations\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "    math_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n",
    "    print(f\"The math implementation runs in {math_time:.3f} microseconds\")\n",
    "\n",
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    try:\n",
    "        flash_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n",
    "        print(f\"The flash attention implementation runs in {flash_time:.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"FlashAttention is not supported. See warnings for reasons.\")\n",
    "\n",
    "with sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):\n",
    "    try:\n",
    "        efficient_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n",
    "        print(f\"The memory efficient implementation runs in {efficient_time:.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"EfficientAttention is not supported. See warnings for reasons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32608354-1ccf-49b6-b926-8148c9939730",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Flash attn native test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af0df696-291b-403b-85a1-7db4136d508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from flash_attn import (\n",
    "    flash_attn_func,\n",
    "    flash_attn_kvpacked_func,\n",
    "    flash_attn_qkvpacked_func,\n",
    "    flash_attn_varlen_func,\n",
    "    flash_attn_varlen_kvpacked_func,\n",
    "    flash_attn_varlen_qkvpacked_func,\n",
    "    flash_attn_with_kvcache,\n",
    ")\n",
    "from flash_attn.bert_padding import pad_input, unpad_input\n",
    "from flash_attn.layers.rotary import apply_rotary_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7ef83f4-5298-4304-9d7d-1bcc1a8d0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_ref(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    query_padding_mask=None,\n",
    "    key_padding_mask=None,\n",
    "    dropout_p=0.0,\n",
    "    dropout_mask=None,\n",
    "    causal=False,\n",
    "    window_size=(-1, -1),  # -1 means infinite window size\n",
    "    upcast=True,\n",
    "    reorder_ops=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        q: (batch_size, seqlen_q, nheads, head_dim)\n",
    "        k: (batch_size, seqlen_k, nheads_k, head_dim)\n",
    "        v: (batch_size, seqlen_k, nheads_k, head_dim)\n",
    "        query_padding_mask: (batch_size, seqlen_q)\n",
    "        key_padding_mask: (batch_size, seqlen_k)\n",
    "        dropout_p: float\n",
    "        dropout_mask: (batch_size, nheads, seqlen_q, seqlen_k)\n",
    "        causal: whether to apply causal masking\n",
    "        window_size: (int, int), left and right window size\n",
    "        upcast: whether to cast all inputs to fp32, do all computation in fp32, then cast\n",
    "            output back to fp16/bf16.\n",
    "        reorder_ops: whether to change the order of operations (scaling k instead of scaling k, etc.)\n",
    "            without changing the math. This is to estimate the numerical error from operation\n",
    "            reordering.\n",
    "    Output:\n",
    "        output: (batch_size, seqlen_q, nheads, head_dim)\n",
    "        attention: (batch_size, nheads, seqlen_q, seqlen_k), softmax after dropout\n",
    "    \"\"\"\n",
    "    if causal:\n",
    "        window_size = (window_size[0], 0)\n",
    "    dtype_og = q.dtype\n",
    "    if upcast:\n",
    "        q, k, v = q.float(), k.float(), v.float()\n",
    "    seqlen_q, seqlen_k = q.shape[1], k.shape[1]\n",
    "    k = repeat(k, \"b s h d -> b s (h g) d\", g=q.shape[2] // k.shape[2])\n",
    "    v = repeat(v, \"b s h d -> b s (h g) d\", g=q.shape[2] // v.shape[2])\n",
    "    d = q.shape[-1]\n",
    "    if not reorder_ops:\n",
    "        scores = torch.einsum(\"bthd,bshd->bhts\", q / math.sqrt(d), k)\n",
    "    else:\n",
    "        scores = torch.einsum(\"bthd,bshd->bhts\", q, k / math.sqrt(d))\n",
    "    if key_padding_mask is not None:\n",
    "        scores.masked_fill_(rearrange(~key_padding_mask, \"b s -> b 1 1 s\"), float(\"-inf\"))\n",
    "    if window_size[0] >= 0 or window_size[1] >= 0:\n",
    "        local_mask = construct_local_mask(\n",
    "            seqlen_q,\n",
    "            seqlen_k,\n",
    "            window_size,\n",
    "            query_padding_mask,\n",
    "            key_padding_mask,\n",
    "            q.device,\n",
    "        )\n",
    "        scores.masked_fill_(local_mask, float(\"-inf\"))\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    # Some rows might be completely masked out so we fill them with zero instead of NaN\n",
    "    if window_size[0] >= 0 or window_size[1] >= 0:\n",
    "        attention = attention.masked_fill(torch.all(local_mask, dim=-1, keepdim=True), 0.0)\n",
    "    # We want to mask here so that the attention matrix doesn't have any NaNs\n",
    "    # Otherwise we'll get NaN in dV\n",
    "    if query_padding_mask is not None:\n",
    "        attention = attention.masked_fill(rearrange(~query_padding_mask, \"b s -> b 1 s 1\"), 0.0)\n",
    "    dropout_scaling = 1.0 / (1 - dropout_p)\n",
    "    # attention_drop = attention.masked_fill(~dropout_mask, 0.0) * dropout_scaling\n",
    "    # output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
    "    if dropout_mask is not None:\n",
    "        attention_drop = attention.masked_fill(~dropout_mask, 0.0)\n",
    "    else:\n",
    "        attention_drop = attention\n",
    "    output = torch.einsum(\"bhts,bshd->bthd\", attention_drop, v * dropout_scaling)\n",
    "    if query_padding_mask is not None:\n",
    "        output.masked_fill_(rearrange(~query_padding_mask, \"b s -> b s 1 1\"), 0.0)\n",
    "    return output.to(dtype=dtype_og), attention.to(dtype=dtype_og)\n",
    "def generate_qkv(\n",
    "    q, k, v, query_padding_mask=None, key_padding_mask=None, kvpacked=False, qkvpacked=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        q: (batch_size, seqlen_q, nheads, d)\n",
    "        k: (batch_size, seqlen_k, nheads_k, d)\n",
    "        v: (batch_size, seqlen_k, nheads_k, d)\n",
    "        query_padding_mask: (batch_size, seqlen), bool\n",
    "        key_padding_mask: (batch_size, seqlen), bool\n",
    "    \"\"\"\n",
    "    assert not (kvpacked and qkvpacked)\n",
    "    batch_size, seqlen_q, nheads, d = q.shape\n",
    "    _, seqlen_k, nheads_k, _ = k.shape\n",
    "    assert k.shape == (batch_size, seqlen_k, nheads_k, d)\n",
    "    assert v.shape == (batch_size, seqlen_k, nheads_k, d)\n",
    "\n",
    "    if query_padding_mask is not None:\n",
    "        q_unpad, indices_q, cu_seqlens_q, max_seqlen_q = unpad_input(q, query_padding_mask)\n",
    "        output_pad_fn = lambda output_unpad: pad_input(output_unpad, indices_q, batch_size, seqlen_q)\n",
    "    else:\n",
    "        q_unpad = rearrange(q, \"b s h d -> (b s) h d\")\n",
    "        cu_seqlens_q = torch.arange(0, (batch_size + 1) * seqlen_q, step=seqlen_q, dtype=torch.int32, device=q_unpad.device)\n",
    "        max_seqlen_q = seqlen_q\n",
    "        output_pad_fn = lambda output_unpad: rearrange(output_unpad, \"(b s) h d -> b s h d\", b=batch_size)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        k_unpad, indices_k, cu_seqlens_k, max_seqlen_k = unpad_input(k, key_padding_mask)\n",
    "        v_unpad, _, _, _ = unpad_input(v, key_padding_mask)\n",
    "    else:\n",
    "        k_unpad = rearrange(k, \"b s h d -> (b s) h d\")\n",
    "        v_unpad = rearrange(v, \"b s h d -> (b s) h d\")\n",
    "        cu_seqlens_k = torch.arange(0, (batch_size + 1) * seqlen_k, step=seqlen_k, dtype=torch.int32, device=k_unpad.device)\n",
    "        max_seqlen_k = seqlen_k\n",
    "\n",
    "    if qkvpacked:\n",
    "        assert (query_padding_mask == key_padding_mask).all()\n",
    "        assert nheads == nheads_k\n",
    "        qkv_unpad = torch.stack([q_unpad, k_unpad, v_unpad], dim=1)\n",
    "        qkv = torch.stack([q, k, v], dim=2)\n",
    "        if query_padding_mask is not None:\n",
    "            dqkv_pad_fn = lambda dqkv_unpad: pad_input(dqkv_unpad, indices_q, batch_size, seqlen_q)\n",
    "        else:\n",
    "            dqkv_pad_fn = lambda dqkv_unpad: rearrange(dqkv_unpad, \"(b s) t h d -> b s t h d\", b=batch_size)\n",
    "        return (\n",
    "            qkv_unpad.detach().requires_grad_(),\n",
    "            cu_seqlens_q,\n",
    "            max_seqlen_q,\n",
    "            qkv.detach().requires_grad_(),\n",
    "            output_pad_fn,\n",
    "            dqkv_pad_fn,\n",
    "        )\n",
    "    elif kvpacked:\n",
    "        kv_unpad = torch.stack([k_unpad, v_unpad], dim=1)\n",
    "        kv = torch.stack([k, v], dim=2)\n",
    "        dq_pad_fn = output_pad_fn\n",
    "        if key_padding_mask is not None:\n",
    "            dkv_pad_fn = lambda dkv_unpad: pad_input(dkv_unpad, indices_k, batch_size, seqlen_k)\n",
    "        else:\n",
    "            dkv_pad_fn = lambda dkv_unpad: rearrange(\n",
    "                dkv_unpad, \"(b s) t h d -> b s t h d\", b=batch_size\n",
    "            )\n",
    "        return (\n",
    "            q_unpad.detach().requires_grad_(),\n",
    "            kv_unpad.detach().requires_grad_(),\n",
    "            cu_seqlens_q,\n",
    "            cu_seqlens_k,\n",
    "            max_seqlen_q,\n",
    "            max_seqlen_k,\n",
    "            q.detach().requires_grad_(),\n",
    "            kv.detach().requires_grad_(),\n",
    "            output_pad_fn,\n",
    "            dq_pad_fn,\n",
    "            dkv_pad_fn,\n",
    "        )\n",
    "    else:\n",
    "        dq_pad_fn = output_pad_fn\n",
    "        if key_padding_mask is not None:\n",
    "            dk_pad_fn = lambda dk_unpad: pad_input(dk_unpad, indices_k, batch_size, seqlen_k)\n",
    "        else:\n",
    "            dk_pad_fn = lambda dk_unpad: rearrange(dk_unpad, \"(b s) h d -> b s h d\", b=batch_size)\n",
    "        return (\n",
    "            q_unpad.detach().requires_grad_(),\n",
    "            k_unpad.detach().requires_grad_(),\n",
    "            v_unpad.detach().requires_grad_(),\n",
    "            cu_seqlens_q,\n",
    "            cu_seqlens_k,\n",
    "            max_seqlen_q,\n",
    "            max_seqlen_k,\n",
    "            q.detach().requires_grad_(),\n",
    "            k.detach().requires_grad_(),\n",
    "            v.detach().requires_grad_(),\n",
    "            output_pad_fn,\n",
    "            dq_pad_fn,\n",
    "            dk_pad_fn,\n",
    "        )\n",
    "def generate_random_padding_mask(max_seqlen, batch_size, device, mode=\"random\"):\n",
    "    assert mode in [\"full\", \"random\", \"third\"]\n",
    "    if mode == \"full\":\n",
    "        lengths = torch.full((batch_size, 1), max_seqlen, device=device, dtype=torch.int32)\n",
    "    elif mode == \"random\":\n",
    "        lengths = torch.randint(\n",
    "            max(1, max_seqlen - 20), max_seqlen + 1, (batch_size, 1), device=device\n",
    "        )\n",
    "    elif mode == \"third\":\n",
    "        lengths = torch.randint(max_seqlen // 3, max_seqlen + 1, (batch_size, 1), device=device)\n",
    "    padding_mask = (\n",
    "        repeat(torch.arange(max_seqlen, device=device), \"s -> b s\", b=batch_size) < lengths\n",
    "    )\n",
    "    return padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bae3cdcc-5752-421f-8db6-8b75be1206cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _upad_input(query_layer, key_layer, value_layer, attention_mask, query_length):\n",
    "        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n",
    "        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n",
    "        \n",
    "        key_layer = index_first_axis(\n",
    "            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n",
    "        )\n",
    "        value_layer = index_first_axis(\n",
    "            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n",
    "        )\n",
    "        if query_length == kv_seq_len:\n",
    "            num_heads = query_layer.shape[-2]\n",
    "            query_layer = index_first_axis(\n",
    "                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n",
    "            )\n",
    "            cu_seqlens_q = cu_seqlens_k\n",
    "            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n",
    "            indices_q = indices_k\n",
    "        elif query_length == 1:\n",
    "            max_seqlen_in_batch_q = 1\n",
    "            cu_seqlens_q = torch.arange(\n",
    "                batch_size + 1, dtype=torch.int32, device=query_layer.device\n",
    "            )  # There is a memcpy here, that is very bad.\n",
    "            indices_q = cu_seqlens_q[:-1]\n",
    "            query_layer = query_layer.squeeze(1)\n",
    "        else:\n",
    "            # The -q_len: slice assumes left padding.\n",
    "            attention_mask = attention_mask[:, -query_length:]\n",
    "            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n",
    "\n",
    "        return (\n",
    "            query_layer,\n",
    "            key_layer,\n",
    "            value_layer,\n",
    "            indices_q,\n",
    "            (cu_seqlens_q, cu_seqlens_k),\n",
    "            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e256ddcc-fe13-466b-a2f0-76cd50dfe859",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ref, attn_ref = attention_ref(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            query_padding_mask,\n",
    "            key_padding_mask,\n",
    "            dropout_p,\n",
    "            dropout_mask,\n",
    "            causal=causal,\n",
    "            window_size=window_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0ad05c66-d680-4186-adae-3a080bdc6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seqlen_q = 10\n",
    "seqlen_k = 213\n",
    "nheads = nheads_k = 16 \n",
    "d = 32\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "q = torch.randn(batch_size, seqlen_q,   nheads, d, device=device, dtype=dtype, requires_grad=True)\n",
    "k = torch.randn(batch_size, seqlen_k, nheads_k, d, device=device, dtype=dtype, requires_grad=True)\n",
    "v = torch.randn(batch_size, seqlen_k, nheads_k, d, device=device, dtype=dtype, requires_grad=True)\n",
    "query_padding_mask = generate_random_padding_mask(seqlen_q, batch_size, device, mode=\"random\")\n",
    "key_padding_mask = None\n",
    "dropout_p = 0\n",
    "causal = False\n",
    "window_size=(-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7b34e753-2e63-4dd9-b4ce-3e22e608fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LLAMA_ATTENTION_CLASSES, LlamaAttention, LlamaMLP, AttentionMaskConverter, Cache, LlamaRMSNorm, StaticCache, apply_rotary_pos_emb, repeat_kv, is_flash_attn_2_available, _get_unpad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4b0a0974-9353-46a8-a8c6-7ad83c4e8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_layer,key_layer,value_layer,indices_q,(cu_seqlens_q, cu_seqlens_k),(max_seqlen_in_batch_q, max_seqlen_in_batch_k) =  _upad_input(q, k, v, query_padding_mask, seqlen_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "76cceebe-6286-4023-b02a-f194f780dfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 213, 16, 32])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b313ecb4-1e5d-41a3-bca6-d80b73e423db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 16, 32])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ff8048e5-0ca3-462a-9754-604863d7b4b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (9) must match the size of tensor b (426) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk_unpad\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9) must match the size of tensor b (426) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "torch.dist(key_layer,k_unpad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c4f03509-438b-48d9-8c17-7adfc0ee8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    q_unpad,\n",
    "    k_unpad,\n",
    "    v_unpad,\n",
    "    cu_seqlens_q,\n",
    "    cu_seqlens_k,\n",
    "    max_seqlen_q,\n",
    "    max_seqlen_k,\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    output_pad_fn,\n",
    "    dq_pad_fn,\n",
    "    dk_pad_fn,\n",
    ") = generate_qkv(q, k, v, query_padding_mask, key_padding_mask, kvpacked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89e229-2b79-4ff1-9c86-ff7ce87e1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_unpad, sm_lse, S_dmask = flash_attn_varlen_func(\n",
    "    q_unpad,\n",
    "    k_unpad,\n",
    "    v_unpad,\n",
    "    cu_seqlens_q,\n",
    "    cu_seqlens_k,\n",
    "    max_seqlen_q,\n",
    "    max_seqlen_k,\n",
    "    dropout_p,\n",
    "    return_attn_probs=True,\n",
    "    causal=causal,\n",
    "    window_size=window_size,\n",
    ")\n",
    "out = output_pad_fn(out_unpad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4b24c723-428a-4f8f-a954-89998cf4bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ref, attn_ref = attention_ref(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    query_padding_mask,\n",
    "    key_padding_mask,\n",
    "    0.0,\n",
    "    None,\n",
    "    causal=causal,\n",
    "    window_size=window_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "123452b3-0849-4020-93dd-a01734737cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 16, 32])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a7ccacb0-a577-4f6c-bb39-8d68ddd2094c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 213, 16, 32])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "773479d3-e2e6-4ef7-b230-f871ff43cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.model.config._attn_implementation = 'eager'\n",
    "attention_mask = decoder.model.get_cross_attn_mask(query_padding_mask, query_states,  key_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6379d9d8-3257-414e-8ff4-84dbfeb16d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10, 213])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2e02a43e-499b-475b-8f91-d1726246c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = q\n",
    "key_states   = k \n",
    "value_states = v\n",
    "attn_weights = torch.einsum(\"bthd,bshd->bhts\", query_states / math.sqrt(d), key_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1d8f685a-af15-4e33-b51c-090972a94910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10, 16])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask  = attention_mask[:, :, :, : key_states.shape[-2]] ##\n",
    "causal_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c451a005-ea09-4c1d-920e-4442bc2e335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if attention_mask is not None:  # no matter the length, we just slice it\n",
    "    causal_mask  = attention_mask[:, :, :, : key_states.shape[1]] ##\n",
    "    attn_weights = attn_weights + causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0e5d2e44-fa34-45f5-ba75-3193bb363724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upcast attention to fp32\n",
    "attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "attn_output  = torch.einsum(\"bhts,bshd->bthd\", attn_weights, value_states)\n",
    "bsz, q_len, _, _ = query_states.shape\n",
    "bsz, v_len, H, d = value_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cc820705-2566-4300-a9c0-8f78badf3701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 16, 32])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "241d24b2-9b09-4494-8eaf-b7e2e15833b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0047, device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(attn_output[query_padding_mask],out_ref[query_padding_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2e7182ef-3779-4ea0-adf3-16f265d3a04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0021, device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(out,out_ref)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

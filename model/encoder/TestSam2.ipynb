{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49cd7f92-4561-44eb-bfee-4d1094adb509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bingxing2/ailab/scxlab0194/.conda/envs/locr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sam2.modeling_sam2 import *\n",
    "from sam2.configuration_sam2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9faad9f5-3e89-4e85-9325-7d1fa04036d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Sam2ImageEncoderConfig(embed_dim=114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b8c42e8-5079-4a85-a57a-458e9a4567cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.load('../../checkpoints/UparxiveV2/FlougatU1KS/flougatU_1KS_bf16_more/best/weight/epoch0018/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f9ebf83-972c-434e-a571-4eed01860352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.image_projection => torch.Size([1024, 768])\n",
      "encoder.vision_tower.convs.0.proj.weight => torch.Size([128, 3, 7, 7])\n",
      "encoder.vision_tower.convs.0.proj.bias => torch.Size([128])\n",
      "encoder.vision_tower.convs.0.norm.weight => torch.Size([128])\n",
      "encoder.vision_tower.convs.0.norm.bias => torch.Size([128])\n",
      "encoder.vision_tower.convs.1.proj.weight => torch.Size([256, 128, 3, 3])\n",
      "encoder.vision_tower.convs.1.proj.bias => torch.Size([256])\n",
      "encoder.vision_tower.convs.1.norm.weight => torch.Size([128])\n",
      "encoder.vision_tower.convs.1.norm.bias => torch.Size([128])\n",
      "encoder.vision_tower.convs.2.proj.weight => torch.Size([512, 256, 3, 3])\n",
      "encoder.vision_tower.convs.2.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.convs.2.norm.weight => torch.Size([256])\n",
      "encoder.vision_tower.convs.2.norm.bias => torch.Size([256])\n",
      "encoder.vision_tower.convs.3.proj.weight => torch.Size([1024, 512, 3, 3])\n",
      "encoder.vision_tower.convs.3.proj.bias => torch.Size([1024])\n",
      "encoder.vision_tower.convs.3.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.convs.3.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.conv1.fn.dw.weight => torch.Size([128, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.conv1.fn.dw.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.window_attn.norm.weight => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.window_attn.norm.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.weight => torch.Size([384, 128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.bias => torch.Size([384])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.window_attn.fn.proj.weight => torch.Size([128, 128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.window_attn.fn.proj.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.conv2.fn.dw.weight => torch.Size([128, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.conv2.fn.dw.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.ffn.norm.weight => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.ffn.norm.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.weight => torch.Size([512, 128])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.weight => torch.Size([128, 512])\n",
      "encoder.vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.conv1.fn.dw.weight => torch.Size([128, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.conv1.fn.dw.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.channel_attn.norm.weight => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.channel_attn.norm.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.weight => torch.Size([384, 128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.bias => torch.Size([384])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.channel_attn.fn.proj.weight => torch.Size([128, 128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.channel_attn.fn.proj.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.conv2.fn.dw.weight => torch.Size([128, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.conv2.fn.dw.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.ffn.norm.weight => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.ffn.norm.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.weight => torch.Size([512, 128])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.weight => torch.Size([128, 512])\n",
      "encoder.vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.bias => torch.Size([128])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.conv1.fn.dw.weight => torch.Size([256, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.conv1.fn.dw.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.window_attn.norm.weight => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.window_attn.norm.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.weight => torch.Size([768, 256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.bias => torch.Size([768])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.window_attn.fn.proj.weight => torch.Size([256, 256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.window_attn.fn.proj.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.conv2.fn.dw.weight => torch.Size([256, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.conv2.fn.dw.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.ffn.norm.weight => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.ffn.norm.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.weight => torch.Size([1024, 256])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.weight => torch.Size([256, 1024])\n",
      "encoder.vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.conv1.fn.dw.weight => torch.Size([256, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.conv1.fn.dw.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.channel_attn.norm.weight => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.channel_attn.norm.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.weight => torch.Size([768, 256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.bias => torch.Size([768])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.channel_attn.fn.proj.weight => torch.Size([256, 256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.channel_attn.fn.proj.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.conv2.fn.dw.weight => torch.Size([256, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.conv2.fn.dw.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.ffn.norm.weight => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.ffn.norm.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.weight => torch.Size([1024, 256])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.weight => torch.Size([256, 1024])\n",
      "encoder.vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.bias => torch.Size([256])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.window_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.window_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.window_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.window_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.channel_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.channel_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.channel_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.channel_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.window_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.window_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.window_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.window_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.channel_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.channel_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.channel_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.channel_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.window_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.window_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.window_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.window_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.channel_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.channel_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.channel_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.channel_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.window_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.window_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.window_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.window_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.channel_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.channel_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.channel_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.channel_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.window_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.window_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.window_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.window_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.channel_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.channel_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.channel_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.channel_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.window_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.window_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.window_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.window_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.channel_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.channel_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.channel_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.channel_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.window_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.window_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.window_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.window_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.channel_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.channel_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.channel_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.channel_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.window_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.window_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.window_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.window_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.channel_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.channel_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.channel_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.channel_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.window_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.window_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.window_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.window_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.conv1.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.conv1.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.channel_attn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.channel_attn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.weight => torch.Size([1536, 512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.bias => torch.Size([1536])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.channel_attn.fn.proj.weight => torch.Size([512, 512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.channel_attn.fn.proj.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.conv2.fn.dw.weight => torch.Size([512, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.conv2.fn.dw.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.ffn.norm.weight => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.ffn.norm.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.weight => torch.Size([2048, 512])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.bias => torch.Size([2048])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.weight => torch.Size([512, 2048])\n",
      "encoder.vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.bias => torch.Size([512])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.conv1.fn.dw.weight => torch.Size([1024, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.conv1.fn.dw.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.window_attn.norm.weight => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.window_attn.norm.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.weight => torch.Size([3072, 1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.bias => torch.Size([3072])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.window_attn.fn.proj.weight => torch.Size([1024, 1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.window_attn.fn.proj.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.conv2.fn.dw.weight => torch.Size([1024, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.conv2.fn.dw.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.ffn.norm.weight => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.ffn.norm.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.weight => torch.Size([4096, 1024])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.bias => torch.Size([4096])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.weight => torch.Size([1024, 4096])\n",
      "encoder.vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.conv1.fn.dw.weight => torch.Size([1024, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.conv1.fn.dw.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.channel_attn.norm.weight => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.channel_attn.norm.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.weight => torch.Size([3072, 1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.bias => torch.Size([3072])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.channel_attn.fn.proj.weight => torch.Size([1024, 1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.channel_attn.fn.proj.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.conv2.fn.dw.weight => torch.Size([1024, 1, 3, 3])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.conv2.fn.dw.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.ffn.norm.weight => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.ffn.norm.bias => torch.Size([1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.weight => torch.Size([4096, 1024])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.bias => torch.Size([4096])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.weight => torch.Size([1024, 4096])\n",
      "encoder.vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.bias => torch.Size([1024])\n",
      "encoder.vision_tower.norms.weight => torch.Size([1024])\n",
      "encoder.vision_tower.norms.bias => torch.Size([1024])\n",
      "encoder.vision_tower.head.weight => torch.Size([1000, 1024])\n",
      "encoder.vision_tower.head.bias => torch.Size([1000])\n",
      "encoder.image_proj_norm.weight => torch.Size([768])\n",
      "encoder.image_proj_norm.bias => torch.Size([768])\n",
      "encoder.image_pos_embed.row_embeddings.weight => torch.Size([50, 512])\n",
      "encoder.image_pos_embed.column_embeddings.weight => torch.Size([50, 512])\n",
      "encoder.visual_temporal_embed.pos_idx_to_embed => torch.Size([100, 1024])\n",
      "decoder.model.img_coord => torch.Size([1, 768, 2, 2])\n",
      "decoder.model.embed_tokens.weight => torch.Size([50232, 768])\n",
      "decoder.model.layers.0.self_attn.q_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.0.self_attn.k_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.0.self_attn.v_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.0.self_attn.o_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.0.cross_attn.q_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.0.cross_attn.k_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.0.cross_attn.v_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.0.cross_attn.o_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.0.mlp.gate_proj.weight => torch.Size([2048, 768])\n",
      "decoder.model.layers.0.mlp.up_proj.weight => torch.Size([2048, 768])\n",
      "decoder.model.layers.0.mlp.down_proj.weight => torch.Size([768, 2048])\n",
      "decoder.model.layers.0.self_attn_layernorm.weight => torch.Size([768])\n",
      "decoder.model.layers.0.cross_attn_layer_norm.weight => torch.Size([768])\n",
      "decoder.model.layers.0.post_attention_layernorm.weight => torch.Size([768])\n",
      "decoder.model.layers.1.self_attn.q_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.1.self_attn.k_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.1.self_attn.v_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.1.self_attn.o_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.1.cross_attn.q_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.1.cross_attn.k_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.1.cross_attn.v_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.1.cross_attn.o_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.1.mlp.gate_proj.weight => torch.Size([2048, 768])\n",
      "decoder.model.layers.1.mlp.up_proj.weight => torch.Size([2048, 768])\n",
      "decoder.model.layers.1.mlp.down_proj.weight => torch.Size([768, 2048])\n",
      "decoder.model.layers.1.self_attn_layernorm.weight => torch.Size([768])\n",
      "decoder.model.layers.1.cross_attn_layer_norm.weight => torch.Size([768])\n",
      "decoder.model.layers.1.post_attention_layernorm.weight => torch.Size([768])\n",
      "decoder.model.layers.2.self_attn.q_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.2.self_attn.k_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.2.self_attn.v_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.2.self_attn.o_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.2.cross_attn.q_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.2.cross_attn.k_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.2.cross_attn.v_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.2.cross_attn.o_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.2.mlp.gate_proj.weight => torch.Size([2048, 768])\n",
      "decoder.model.layers.2.mlp.up_proj.weight => torch.Size([2048, 768])\n",
      "decoder.model.layers.2.mlp.down_proj.weight => torch.Size([768, 2048])\n",
      "decoder.model.layers.2.self_attn_layernorm.weight => torch.Size([768])\n",
      "decoder.model.layers.2.cross_attn_layer_norm.weight => torch.Size([768])\n",
      "decoder.model.layers.2.post_attention_layernorm.weight => torch.Size([768])\n",
      "decoder.model.layers.3.self_attn.q_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.3.self_attn.k_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.3.self_attn.v_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.3.self_attn.o_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.3.cross_attn.q_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.3.cross_attn.k_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.3.cross_attn.v_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.3.cross_attn.o_proj.weight => torch.Size([768, 768])\n",
      "decoder.model.layers.3.mlp.gate_proj.weight => torch.Size([2048, 768])\n",
      "decoder.model.layers.3.mlp.up_proj.weight => torch.Size([2048, 768])\n",
      "decoder.model.layers.3.mlp.down_proj.weight => torch.Size([768, 2048])\n",
      "decoder.model.layers.3.self_attn_layernorm.weight => torch.Size([768])\n",
      "decoder.model.layers.3.cross_attn_layer_norm.weight => torch.Size([768])\n",
      "decoder.model.layers.3.post_attention_layernorm.weight => torch.Size([768])\n",
      "decoder.model.norm.weight => torch.Size([768])\n",
      "decoder.model.prompt_encoder.pe_layer.positional_encoding_gaussian_matrix => torch.Size([2, 384])\n",
      "decoder.model.prompt_encoder.point_embeddings.0.weight => torch.Size([1, 768])\n",
      "decoder.model.prompt_encoder.point_embeddings.1.weight => torch.Size([1, 768])\n",
      "decoder.lm_head.weight => torch.Size([50232, 768])\n",
      "decoder.position_attn.q_proj.weight => torch.Size([768, 768])\n",
      "decoder.position_attn.k_proj.weight => torch.Size([768, 768])\n",
      "decoder.position_attn.v_proj.weight => torch.Size([768, 768])\n",
      "decoder.position_attn.o_proj.0.weight => torch.Size([768])\n",
      "decoder.position_attn.o_proj.0.bias => torch.Size([768])\n",
      "decoder.position_attn.o_proj.1.weight => torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "for k, v in weight.items():\n",
    "    print(f\"{k} => {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15980447-4048-4bc7-84a2-b245537f6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_weight = {}\n",
    "for key, val in weight.items():\n",
    "    if key.startswith(\"decoder.\"):\n",
    "        decoder_weight[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d22c1087-e074-49a6-b03b-77ec89bf837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder_weight|decoder_weight, \"../../pretrain_weights/slougat.start.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c24071e-d450-4cb7-9b32-0ac1f87d7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_weight = encoder_weight|decoder_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45dd4d27-3533-4784-8584-b5670796ca96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder.neck.convs.0.conv.bias', 'encoder.neck.convs.0.conv.weight', 'encoder.neck.convs.1.conv.bias', 'encoder.neck.convs.1.conv.weight', 'encoder.neck.convs.2.conv.bias', 'encoder.neck.convs.2.conv.weight', 'encoder.neck.convs.3.conv.bias', 'encoder.neck.convs.3.conv.weight', 'encoder.trunk.blocks.0.attn.proj.bias', 'encoder.trunk.blocks.0.attn.proj.weight', 'encoder.trunk.blocks.0.attn.qkv.bias', 'encoder.trunk.blocks.0.attn.qkv.weight', 'encoder.trunk.blocks.0.mlp.layers.0.bias', 'encoder.trunk.blocks.0.mlp.layers.0.weight', 'encoder.trunk.blocks.0.mlp.layers.1.bias', 'encoder.trunk.blocks.0.mlp.layers.1.weight', 'encoder.trunk.blocks.0.norm1.bias', 'encoder.trunk.blocks.0.norm1.weight', 'encoder.trunk.blocks.0.norm2.bias', 'encoder.trunk.blocks.0.norm2.weight', 'encoder.trunk.blocks.1.attn.proj.bias', 'encoder.trunk.blocks.1.attn.proj.weight', 'encoder.trunk.blocks.1.attn.qkv.bias', 'encoder.trunk.blocks.1.attn.qkv.weight', 'encoder.trunk.blocks.1.mlp.layers.0.bias', 'encoder.trunk.blocks.1.mlp.layers.0.weight', 'encoder.trunk.blocks.1.mlp.layers.1.bias', 'encoder.trunk.blocks.1.mlp.layers.1.weight', 'encoder.trunk.blocks.1.norm1.bias', 'encoder.trunk.blocks.1.norm1.weight', 'encoder.trunk.blocks.1.norm2.bias', 'encoder.trunk.blocks.1.norm2.weight', 'encoder.trunk.blocks.10.attn.proj.bias', 'encoder.trunk.blocks.10.attn.proj.weight', 'encoder.trunk.blocks.10.attn.qkv.bias', 'encoder.trunk.blocks.10.attn.qkv.weight', 'encoder.trunk.blocks.10.mlp.layers.0.bias', 'encoder.trunk.blocks.10.mlp.layers.0.weight', 'encoder.trunk.blocks.10.mlp.layers.1.bias', 'encoder.trunk.blocks.10.mlp.layers.1.weight', 'encoder.trunk.blocks.10.norm1.bias', 'encoder.trunk.blocks.10.norm1.weight', 'encoder.trunk.blocks.10.norm2.bias', 'encoder.trunk.blocks.10.norm2.weight', 'encoder.trunk.blocks.11.attn.proj.bias', 'encoder.trunk.blocks.11.attn.proj.weight', 'encoder.trunk.blocks.11.attn.qkv.bias', 'encoder.trunk.blocks.11.attn.qkv.weight', 'encoder.trunk.blocks.11.mlp.layers.0.bias', 'encoder.trunk.blocks.11.mlp.layers.0.weight', 'encoder.trunk.blocks.11.mlp.layers.1.bias', 'encoder.trunk.blocks.11.mlp.layers.1.weight', 'encoder.trunk.blocks.11.norm1.bias', 'encoder.trunk.blocks.11.norm1.weight', 'encoder.trunk.blocks.11.norm2.bias', 'encoder.trunk.blocks.11.norm2.weight', 'encoder.trunk.blocks.12.attn.proj.bias', 'encoder.trunk.blocks.12.attn.proj.weight', 'encoder.trunk.blocks.12.attn.qkv.bias', 'encoder.trunk.blocks.12.attn.qkv.weight', 'encoder.trunk.blocks.12.mlp.layers.0.bias', 'encoder.trunk.blocks.12.mlp.layers.0.weight', 'encoder.trunk.blocks.12.mlp.layers.1.bias', 'encoder.trunk.blocks.12.mlp.layers.1.weight', 'encoder.trunk.blocks.12.norm1.bias', 'encoder.trunk.blocks.12.norm1.weight', 'encoder.trunk.blocks.12.norm2.bias', 'encoder.trunk.blocks.12.norm2.weight', 'encoder.trunk.blocks.13.attn.proj.bias', 'encoder.trunk.blocks.13.attn.proj.weight', 'encoder.trunk.blocks.13.attn.qkv.bias', 'encoder.trunk.blocks.13.attn.qkv.weight', 'encoder.trunk.blocks.13.mlp.layers.0.bias', 'encoder.trunk.blocks.13.mlp.layers.0.weight', 'encoder.trunk.blocks.13.mlp.layers.1.bias', 'encoder.trunk.blocks.13.mlp.layers.1.weight', 'encoder.trunk.blocks.13.norm1.bias', 'encoder.trunk.blocks.13.norm1.weight', 'encoder.trunk.blocks.13.norm2.bias', 'encoder.trunk.blocks.13.norm2.weight', 'encoder.trunk.blocks.14.attn.proj.bias', 'encoder.trunk.blocks.14.attn.proj.weight', 'encoder.trunk.blocks.14.attn.qkv.bias', 'encoder.trunk.blocks.14.attn.qkv.weight', 'encoder.trunk.blocks.14.mlp.layers.0.bias', 'encoder.trunk.blocks.14.mlp.layers.0.weight', 'encoder.trunk.blocks.14.mlp.layers.1.bias', 'encoder.trunk.blocks.14.mlp.layers.1.weight', 'encoder.trunk.blocks.14.norm1.bias', 'encoder.trunk.blocks.14.norm1.weight', 'encoder.trunk.blocks.14.norm2.bias', 'encoder.trunk.blocks.14.norm2.weight', 'encoder.trunk.blocks.15.attn.proj.bias', 'encoder.trunk.blocks.15.attn.proj.weight', 'encoder.trunk.blocks.15.attn.qkv.bias', 'encoder.trunk.blocks.15.attn.qkv.weight', 'encoder.trunk.blocks.15.mlp.layers.0.bias', 'encoder.trunk.blocks.15.mlp.layers.0.weight', 'encoder.trunk.blocks.15.mlp.layers.1.bias', 'encoder.trunk.blocks.15.mlp.layers.1.weight', 'encoder.trunk.blocks.15.norm1.bias', 'encoder.trunk.blocks.15.norm1.weight', 'encoder.trunk.blocks.15.norm2.bias', 'encoder.trunk.blocks.15.norm2.weight', 'encoder.trunk.blocks.16.attn.proj.bias', 'encoder.trunk.blocks.16.attn.proj.weight', 'encoder.trunk.blocks.16.attn.qkv.bias', 'encoder.trunk.blocks.16.attn.qkv.weight', 'encoder.trunk.blocks.16.mlp.layers.0.bias', 'encoder.trunk.blocks.16.mlp.layers.0.weight', 'encoder.trunk.blocks.16.mlp.layers.1.bias', 'encoder.trunk.blocks.16.mlp.layers.1.weight', 'encoder.trunk.blocks.16.norm1.bias', 'encoder.trunk.blocks.16.norm1.weight', 'encoder.trunk.blocks.16.norm2.bias', 'encoder.trunk.blocks.16.norm2.weight', 'encoder.trunk.blocks.17.attn.proj.bias', 'encoder.trunk.blocks.17.attn.proj.weight', 'encoder.trunk.blocks.17.attn.qkv.bias', 'encoder.trunk.blocks.17.attn.qkv.weight', 'encoder.trunk.blocks.17.mlp.layers.0.bias', 'encoder.trunk.blocks.17.mlp.layers.0.weight', 'encoder.trunk.blocks.17.mlp.layers.1.bias', 'encoder.trunk.blocks.17.mlp.layers.1.weight', 'encoder.trunk.blocks.17.norm1.bias', 'encoder.trunk.blocks.17.norm1.weight', 'encoder.trunk.blocks.17.norm2.bias', 'encoder.trunk.blocks.17.norm2.weight', 'encoder.trunk.blocks.18.attn.proj.bias', 'encoder.trunk.blocks.18.attn.proj.weight', 'encoder.trunk.blocks.18.attn.qkv.bias', 'encoder.trunk.blocks.18.attn.qkv.weight', 'encoder.trunk.blocks.18.mlp.layers.0.bias', 'encoder.trunk.blocks.18.mlp.layers.0.weight', 'encoder.trunk.blocks.18.mlp.layers.1.bias', 'encoder.trunk.blocks.18.mlp.layers.1.weight', 'encoder.trunk.blocks.18.norm1.bias', 'encoder.trunk.blocks.18.norm1.weight', 'encoder.trunk.blocks.18.norm2.bias', 'encoder.trunk.blocks.18.norm2.weight', 'encoder.trunk.blocks.19.attn.proj.bias', 'encoder.trunk.blocks.19.attn.proj.weight', 'encoder.trunk.blocks.19.attn.qkv.bias', 'encoder.trunk.blocks.19.attn.qkv.weight', 'encoder.trunk.blocks.19.mlp.layers.0.bias', 'encoder.trunk.blocks.19.mlp.layers.0.weight', 'encoder.trunk.blocks.19.mlp.layers.1.bias', 'encoder.trunk.blocks.19.mlp.layers.1.weight', 'encoder.trunk.blocks.19.norm1.bias', 'encoder.trunk.blocks.19.norm1.weight', 'encoder.trunk.blocks.19.norm2.bias', 'encoder.trunk.blocks.19.norm2.weight', 'encoder.trunk.blocks.2.attn.proj.bias', 'encoder.trunk.blocks.2.attn.proj.weight', 'encoder.trunk.blocks.2.attn.qkv.bias', 'encoder.trunk.blocks.2.attn.qkv.weight', 'encoder.trunk.blocks.2.mlp.layers.0.bias', 'encoder.trunk.blocks.2.mlp.layers.0.weight', 'encoder.trunk.blocks.2.mlp.layers.1.bias', 'encoder.trunk.blocks.2.mlp.layers.1.weight', 'encoder.trunk.blocks.2.norm1.bias', 'encoder.trunk.blocks.2.norm1.weight', 'encoder.trunk.blocks.2.norm2.bias', 'encoder.trunk.blocks.2.norm2.weight', 'encoder.trunk.blocks.2.proj.bias', 'encoder.trunk.blocks.2.proj.weight', 'encoder.trunk.blocks.20.attn.proj.bias', 'encoder.trunk.blocks.20.attn.proj.weight', 'encoder.trunk.blocks.20.attn.qkv.bias', 'encoder.trunk.blocks.20.attn.qkv.weight', 'encoder.trunk.blocks.20.mlp.layers.0.bias', 'encoder.trunk.blocks.20.mlp.layers.0.weight', 'encoder.trunk.blocks.20.mlp.layers.1.bias', 'encoder.trunk.blocks.20.mlp.layers.1.weight', 'encoder.trunk.blocks.20.norm1.bias', 'encoder.trunk.blocks.20.norm1.weight', 'encoder.trunk.blocks.20.norm2.bias', 'encoder.trunk.blocks.20.norm2.weight', 'encoder.trunk.blocks.21.attn.proj.bias', 'encoder.trunk.blocks.21.attn.proj.weight', 'encoder.trunk.blocks.21.attn.qkv.bias', 'encoder.trunk.blocks.21.attn.qkv.weight', 'encoder.trunk.blocks.21.mlp.layers.0.bias', 'encoder.trunk.blocks.21.mlp.layers.0.weight', 'encoder.trunk.blocks.21.mlp.layers.1.bias', 'encoder.trunk.blocks.21.mlp.layers.1.weight', 'encoder.trunk.blocks.21.norm1.bias', 'encoder.trunk.blocks.21.norm1.weight', 'encoder.trunk.blocks.21.norm2.bias', 'encoder.trunk.blocks.21.norm2.weight', 'encoder.trunk.blocks.21.proj.bias', 'encoder.trunk.blocks.21.proj.weight', 'encoder.trunk.blocks.22.attn.proj.bias', 'encoder.trunk.blocks.22.attn.proj.weight', 'encoder.trunk.blocks.22.attn.qkv.bias', 'encoder.trunk.blocks.22.attn.qkv.weight', 'encoder.trunk.blocks.22.mlp.layers.0.bias', 'encoder.trunk.blocks.22.mlp.layers.0.weight', 'encoder.trunk.blocks.22.mlp.layers.1.bias', 'encoder.trunk.blocks.22.mlp.layers.1.weight', 'encoder.trunk.blocks.22.norm1.bias', 'encoder.trunk.blocks.22.norm1.weight', 'encoder.trunk.blocks.22.norm2.bias', 'encoder.trunk.blocks.22.norm2.weight', 'encoder.trunk.blocks.23.attn.proj.bias', 'encoder.trunk.blocks.23.attn.proj.weight', 'encoder.trunk.blocks.23.attn.qkv.bias', 'encoder.trunk.blocks.23.attn.qkv.weight', 'encoder.trunk.blocks.23.mlp.layers.0.bias', 'encoder.trunk.blocks.23.mlp.layers.0.weight', 'encoder.trunk.blocks.23.mlp.layers.1.bias', 'encoder.trunk.blocks.23.mlp.layers.1.weight', 'encoder.trunk.blocks.23.norm1.bias', 'encoder.trunk.blocks.23.norm1.weight', 'encoder.trunk.blocks.23.norm2.bias', 'encoder.trunk.blocks.23.norm2.weight', 'encoder.trunk.blocks.3.attn.proj.bias', 'encoder.trunk.blocks.3.attn.proj.weight', 'encoder.trunk.blocks.3.attn.qkv.bias', 'encoder.trunk.blocks.3.attn.qkv.weight', 'encoder.trunk.blocks.3.mlp.layers.0.bias', 'encoder.trunk.blocks.3.mlp.layers.0.weight', 'encoder.trunk.blocks.3.mlp.layers.1.bias', 'encoder.trunk.blocks.3.mlp.layers.1.weight', 'encoder.trunk.blocks.3.norm1.bias', 'encoder.trunk.blocks.3.norm1.weight', 'encoder.trunk.blocks.3.norm2.bias', 'encoder.trunk.blocks.3.norm2.weight', 'encoder.trunk.blocks.4.attn.proj.bias', 'encoder.trunk.blocks.4.attn.proj.weight', 'encoder.trunk.blocks.4.attn.qkv.bias', 'encoder.trunk.blocks.4.attn.qkv.weight', 'encoder.trunk.blocks.4.mlp.layers.0.bias', 'encoder.trunk.blocks.4.mlp.layers.0.weight', 'encoder.trunk.blocks.4.mlp.layers.1.bias', 'encoder.trunk.blocks.4.mlp.layers.1.weight', 'encoder.trunk.blocks.4.norm1.bias', 'encoder.trunk.blocks.4.norm1.weight', 'encoder.trunk.blocks.4.norm2.bias', 'encoder.trunk.blocks.4.norm2.weight', 'encoder.trunk.blocks.5.attn.proj.bias', 'encoder.trunk.blocks.5.attn.proj.weight', 'encoder.trunk.blocks.5.attn.qkv.bias', 'encoder.trunk.blocks.5.attn.qkv.weight', 'encoder.trunk.blocks.5.mlp.layers.0.bias', 'encoder.trunk.blocks.5.mlp.layers.0.weight', 'encoder.trunk.blocks.5.mlp.layers.1.bias', 'encoder.trunk.blocks.5.mlp.layers.1.weight', 'encoder.trunk.blocks.5.norm1.bias', 'encoder.trunk.blocks.5.norm1.weight', 'encoder.trunk.blocks.5.norm2.bias', 'encoder.trunk.blocks.5.norm2.weight', 'encoder.trunk.blocks.5.proj.bias', 'encoder.trunk.blocks.5.proj.weight', 'encoder.trunk.blocks.6.attn.proj.bias', 'encoder.trunk.blocks.6.attn.proj.weight', 'encoder.trunk.blocks.6.attn.qkv.bias', 'encoder.trunk.blocks.6.attn.qkv.weight', 'encoder.trunk.blocks.6.mlp.layers.0.bias', 'encoder.trunk.blocks.6.mlp.layers.0.weight', 'encoder.trunk.blocks.6.mlp.layers.1.bias', 'encoder.trunk.blocks.6.mlp.layers.1.weight', 'encoder.trunk.blocks.6.norm1.bias', 'encoder.trunk.blocks.6.norm1.weight', 'encoder.trunk.blocks.6.norm2.bias', 'encoder.trunk.blocks.6.norm2.weight', 'encoder.trunk.blocks.7.attn.proj.bias', 'encoder.trunk.blocks.7.attn.proj.weight', 'encoder.trunk.blocks.7.attn.qkv.bias', 'encoder.trunk.blocks.7.attn.qkv.weight', 'encoder.trunk.blocks.7.mlp.layers.0.bias', 'encoder.trunk.blocks.7.mlp.layers.0.weight', 'encoder.trunk.blocks.7.mlp.layers.1.bias', 'encoder.trunk.blocks.7.mlp.layers.1.weight', 'encoder.trunk.blocks.7.norm1.bias', 'encoder.trunk.blocks.7.norm1.weight', 'encoder.trunk.blocks.7.norm2.bias', 'encoder.trunk.blocks.7.norm2.weight', 'encoder.trunk.blocks.8.attn.proj.bias', 'encoder.trunk.blocks.8.attn.proj.weight', 'encoder.trunk.blocks.8.attn.qkv.bias', 'encoder.trunk.blocks.8.attn.qkv.weight', 'encoder.trunk.blocks.8.mlp.layers.0.bias', 'encoder.trunk.blocks.8.mlp.layers.0.weight', 'encoder.trunk.blocks.8.mlp.layers.1.bias', 'encoder.trunk.blocks.8.mlp.layers.1.weight', 'encoder.trunk.blocks.8.norm1.bias', 'encoder.trunk.blocks.8.norm1.weight', 'encoder.trunk.blocks.8.norm2.bias', 'encoder.trunk.blocks.8.norm2.weight', 'encoder.trunk.blocks.9.attn.proj.bias', 'encoder.trunk.blocks.9.attn.proj.weight', 'encoder.trunk.blocks.9.attn.qkv.bias', 'encoder.trunk.blocks.9.attn.qkv.weight', 'encoder.trunk.blocks.9.mlp.layers.0.bias', 'encoder.trunk.blocks.9.mlp.layers.0.weight', 'encoder.trunk.blocks.9.mlp.layers.1.bias', 'encoder.trunk.blocks.9.mlp.layers.1.weight', 'encoder.trunk.blocks.9.norm1.bias', 'encoder.trunk.blocks.9.norm1.weight', 'encoder.trunk.blocks.9.norm2.bias', 'encoder.trunk.blocks.9.norm2.weight', 'encoder.trunk.patch_embed.proj.bias', 'encoder.trunk.patch_embed.proj.weight', 'encoder.trunk.pos_embed', 'encoder.trunk.pos_embed_window', 'decoder.model.img_coord', 'decoder.model.embed_tokens.weight', 'decoder.model.layers.0.self_attn.q_proj.weight', 'decoder.model.layers.0.self_attn.k_proj.weight', 'decoder.model.layers.0.self_attn.v_proj.weight', 'decoder.model.layers.0.self_attn.o_proj.weight', 'decoder.model.layers.0.cross_attn.q_proj.weight', 'decoder.model.layers.0.cross_attn.k_proj.weight', 'decoder.model.layers.0.cross_attn.v_proj.weight', 'decoder.model.layers.0.cross_attn.o_proj.weight', 'decoder.model.layers.0.mlp.gate_proj.weight', 'decoder.model.layers.0.mlp.up_proj.weight', 'decoder.model.layers.0.mlp.down_proj.weight', 'decoder.model.layers.0.self_attn_layernorm.weight', 'decoder.model.layers.0.cross_attn_layer_norm.weight', 'decoder.model.layers.0.post_attention_layernorm.weight', 'decoder.model.layers.1.self_attn.q_proj.weight', 'decoder.model.layers.1.self_attn.k_proj.weight', 'decoder.model.layers.1.self_attn.v_proj.weight', 'decoder.model.layers.1.self_attn.o_proj.weight', 'decoder.model.layers.1.cross_attn.q_proj.weight', 'decoder.model.layers.1.cross_attn.k_proj.weight', 'decoder.model.layers.1.cross_attn.v_proj.weight', 'decoder.model.layers.1.cross_attn.o_proj.weight', 'decoder.model.layers.1.mlp.gate_proj.weight', 'decoder.model.layers.1.mlp.up_proj.weight', 'decoder.model.layers.1.mlp.down_proj.weight', 'decoder.model.layers.1.self_attn_layernorm.weight', 'decoder.model.layers.1.cross_attn_layer_norm.weight', 'decoder.model.layers.1.post_attention_layernorm.weight', 'decoder.model.layers.2.self_attn.q_proj.weight', 'decoder.model.layers.2.self_attn.k_proj.weight', 'decoder.model.layers.2.self_attn.v_proj.weight', 'decoder.model.layers.2.self_attn.o_proj.weight', 'decoder.model.layers.2.cross_attn.q_proj.weight', 'decoder.model.layers.2.cross_attn.k_proj.weight', 'decoder.model.layers.2.cross_attn.v_proj.weight', 'decoder.model.layers.2.cross_attn.o_proj.weight', 'decoder.model.layers.2.mlp.gate_proj.weight', 'decoder.model.layers.2.mlp.up_proj.weight', 'decoder.model.layers.2.mlp.down_proj.weight', 'decoder.model.layers.2.self_attn_layernorm.weight', 'decoder.model.layers.2.cross_attn_layer_norm.weight', 'decoder.model.layers.2.post_attention_layernorm.weight', 'decoder.model.layers.3.self_attn.q_proj.weight', 'decoder.model.layers.3.self_attn.k_proj.weight', 'decoder.model.layers.3.self_attn.v_proj.weight', 'decoder.model.layers.3.self_attn.o_proj.weight', 'decoder.model.layers.3.cross_attn.q_proj.weight', 'decoder.model.layers.3.cross_attn.k_proj.weight', 'decoder.model.layers.3.cross_attn.v_proj.weight', 'decoder.model.layers.3.cross_attn.o_proj.weight', 'decoder.model.layers.3.mlp.gate_proj.weight', 'decoder.model.layers.3.mlp.up_proj.weight', 'decoder.model.layers.3.mlp.down_proj.weight', 'decoder.model.layers.3.self_attn_layernorm.weight', 'decoder.model.layers.3.cross_attn_layer_norm.weight', 'decoder.model.layers.3.post_attention_layernorm.weight', 'decoder.model.norm.weight', 'decoder.model.prompt_encoder.pe_layer.positional_encoding_gaussian_matrix', 'decoder.model.prompt_encoder.point_embeddings.0.weight', 'decoder.model.prompt_encoder.point_embeddings.1.weight', 'decoder.lm_head.weight', 'decoder.position_attn.q_proj.weight', 'decoder.position_attn.k_proj.weight', 'decoder.position_attn.v_proj.weight', 'decoder.position_attn.o_proj.0.weight', 'decoder.position_attn.o_proj.0.bias', 'decoder.position_attn.o_proj.1.weight'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_weight.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea1440b-3009-429e-82ba-a5a9a2424f32",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Channel dims of trunk and neck do not match. Trunk: [1776, 888, 444, 222], neck: [896, 448, 224, 112]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSam2ImageEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/PromptNougat/model/encoder/sam2/modeling_sam2.py:1350\u001b[0m, in \u001b[0;36mSam2ImageEncoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck \u001b[38;5;241m=\u001b[39m Sam2VisionNeck(config)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscalp\n\u001b[0;32m-> 1350\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk\u001b[38;5;241m.\u001b[39mchannel_list \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck\u001b[38;5;241m.\u001b[39mbackbone_channel_list\n\u001b[1;32m   1352\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChannel dims of trunk and neck do not match. Trunk: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk\u001b[38;5;241m.\u001b[39mchannel_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, neck: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck\u001b[38;5;241m.\u001b[39mbackbone_channel_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Channel dims of trunk and neck do not match. Trunk: [1776, 888, 444, 222], neck: [896, 448, 224, 112]"
     ]
    }
   ],
   "source": [
    "model = Sam2ImageEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9381c3db-74bf-4ff8-998e-38639bb966b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "path = \"../../pretrain_weights/sam2/model.safetensors\"\n",
    "weight = load_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3bd8f74-73f3-4f47-a373-67745430f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_weight = {}\n",
    "for key, val in weight.items():\n",
    "    if key.startswith(\"image_encoder.\"):\n",
    "        key = \"encoder.sam_image_encoder.\"+key[len(\"image_encoder.\"):]\n",
    "        encoder_weight[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82a8fded-6db3-4734-9f92-8ebe7eec013d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sam2ImageEncoder:\n\tsize mismatch for neck.convs.0.conv.weight: copying a param with shape torch.Size([256, 896, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 896, 1, 1]).\n\tsize mismatch for neck.convs.0.conv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for neck.convs.1.conv.weight: copying a param with shape torch.Size([256, 448, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 448, 1, 1]).\n\tsize mismatch for neck.convs.1.conv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for neck.convs.2.conv.weight: copying a param with shape torch.Size([256, 224, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 224, 1, 1]).\n\tsize mismatch for neck.convs.2.conv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for neck.convs.3.conv.weight: copying a param with shape torch.Size([256, 112, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 112, 1, 1]).\n\tsize mismatch for neck.convs.3.conv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/locr/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sam2ImageEncoder:\n\tsize mismatch for neck.convs.0.conv.weight: copying a param with shape torch.Size([256, 896, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 896, 1, 1]).\n\tsize mismatch for neck.convs.0.conv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for neck.convs.1.conv.weight: copying a param with shape torch.Size([256, 448, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 448, 1, 1]).\n\tsize mismatch for neck.convs.1.conv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for neck.convs.2.conv.weight: copying a param with shape torch.Size([256, 224, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 224, 1, 1]).\n\tsize mismatch for neck.convs.2.conv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for neck.convs.3.conv.weight: copying a param with shape torch.Size([256, 112, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 112, 1, 1]).\n\tsize mismatch for neck.convs.3.conv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(encoder_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24799d08-2c87-437d-ab4b-3369cd71f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder_weight, \"../../pretrain_weights/sam2_image_encoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad73632d-deec-4193-ac2e-cbc9b23c3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(1,3,1024,768).cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04937d66-c309-427d-8ed5-849393079627",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa4caea6-abe2-4340-b461-3d708756c47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vision_features', 'vision_pos_enc', 'backbone_fpn'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7d9503-c1c6-46ca-b2ef-cd5c5256e42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64, 48])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['vision_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d282efd4-b1d2-4956-85cd-8b99a2f1e48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64, 48])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['vision_pos_enc'][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97fe23b8-7748-4dd4-ac31-a39603cdf37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64, 48])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['backbone_fpn'][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba3bfebe-277b-4bce-985c-3249dffdf2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_features => torch.Size([1, 256, 64, 64])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model(image)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "    for k, v in out.items():\n",
    "        print(f\"{k} => {v.shape}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08487cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd586d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799ee432-4a44-4872-b9b7-d5f976a38a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhangtianning.di/projects_local/PromptNougat\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170177c3-6417-4132-912b-b03c3ad12ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_arguements import FlougatSmallConfig,UpArouGatSmallConfig\n",
    "from model.model_arguements import TrainerModelConfig\n",
    "from model.trainer_model import PromptNougatModel, LlougatTrainerModel\n",
    "from model.replace_flash_attn import replace_promptdecoder_attn_with_flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0421d9-014b-4e86-bd9f-ac6120981750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:the dimension in encoder=768 is different from decoder=1024, we force the decoder dim to 768\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from simple_parsing import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_arguments(UpArouGatSmallConfig, dest=\"config\")\n",
    "config = parser.parse_args([\"--attn_implementation\",\"eager\"]).config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3f20d7-00ae-4f40-812d-99fb5ca5d9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model =  LlougatTrainerModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f68d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7103374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.load(\"input.pt\",map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7201da88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(nan, grad_fn=<DivBackward0>),\n",
       " {'start_token_loss': tensor(nan, grad_fn=<NllLossBackward0>),\n",
       "  'start_bbox_diou': tensor(nan, grad_fn=<DivBackward0>),\n",
       "  'start_bbox_iou': tensor(nan, grad_fn=<DivBackward0>),\n",
       "  'token_loss': tensor(nan, grad_fn=<NllLossBackward0>),\n",
       "  'bbox_diou': tensor(nan, grad_fn=<DivBackward0>),\n",
       "  'bbox_iou': tensor(nan, grad_fn=<DivBackward0>)},\n",
       " (tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "  \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "  \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "  \n",
       "          [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           ...,\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan],\n",
       "           [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<UnsafeViewBackward0>),\n",
       "  tensor([[[[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]]],\n",
       "  \n",
       "  \n",
       "          [[[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]]],\n",
       "  \n",
       "  \n",
       "          [[[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]]],\n",
       "  \n",
       "  \n",
       "          [[[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]],\n",
       "  \n",
       "           [[nan, nan],\n",
       "            [nan, nan]]]], grad_fn=<StackBackward0>),\n",
       "  DynamicCache()))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9027487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12582912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4* 4096* 16* 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e38286a-b50a-40cd-a50a-977b632ef303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "old_weight = torch.load(\"pretrain_weight/models--microsoft--Florence-2-base-ft/snapshots/e7a5acc73559546de6e12ec0319cd7cc1fa2437c/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dcd498-89d3-4bc6-b9bf-58b5abe94a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_keys = set(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c11f71b-284e-47c5-8d6b-4f5dcbe7f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d730b112-d0da-4057-987d-d6ed1e4029fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in old_weight.keys():\n",
    "    vision_key = \"encoder.\"+key\n",
    "    if vision_key in state_dict:\n",
    "        state_dict[vision_key] = old_weight[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87eff451-d3e1-44ce-ac38-4ca762f1b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dict, \"pretrain_weight/FlougatSmall_start.model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f142e901-e62f-4a4d-af11-fdc92993983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlougatForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlougatModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    }
   ],
   "source": [
    "from decoder.llougat.modeling_llougat import *\n",
    "hidden_size = 32\n",
    "config = LlougatConfig(image_size = [896,672], image_embedding_size=[28, 21],num_hidden_layers=2,\n",
    "                       num_attention_heads=16,num_key_value_heads=16,intermediate_size=1024,\n",
    "                       hidden_size=hidden_size, _attn_implementation='flash_attention_2')\n",
    "decoder = LlougatForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f6c353e-fa64-42b5-a149-1005de385136",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder.cuda()\n",
    "encoder_hidden_states  = torch.randn(2, 28*21, hidden_size)\n",
    "input_ids    = torch.randint(0,32,(2, 10))\n",
    "input_bboxes = torch.randn((2, 10, 4))\n",
    "input_ids[0][5:]=0\n",
    "input_bboxes[0][5:]*=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1227d342-4a3f-4226-b6f3-1943e1d91149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/ztn/projects/PromptNougat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tntorch/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d646cd99-8b92-403b-8798-f8c131307964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch = torch.load('input.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7f6c86-5e84-4b5c-9efa-9c926f5ae72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0, 42,  ...,  1,  1,  1],\n",
       "        [ 0,  0, 69,  ...,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['pre_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4ecc57-583c-4051-b335-4b6f790e0a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1728, 0.1565],\n",
       "         [0.2193, 0.1707]],\n",
       "\n",
       "        [[0.1728, 0.1565],\n",
       "         [0.2193, 0.1707]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['prompt_in'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b07d4-090f-4169-8475-444d2f06e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeba927-faa9-47d6-bb7c-a987b1322e20",
   "metadata": {},
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d155206a-351d-401d-8a48-07d73b24addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.decoder.llougat.modeling_llougat import *\n",
    "hidden_size = 32\n",
    "config = LlougatConfig(image_size = [896,672], image_embedding_size=[28, 21],num_hidden_layers=2,\n",
    "                       num_attention_heads=16,num_key_value_heads=16,intermediate_size=1024,\n",
    "                       hidden_size=hidden_size, _attn_implementation='eager',coordinate_retreive_method=\"heatmap_mapping\")\n",
    "decoder = LlougatForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25c03c0-5ec6-4edb-8a25-6001b117a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use position decay == 1, please note it is not the best setting for generation\n"
     ]
    }
   ],
   "source": [
    "from model.decoder.locr.modeling_locr import *\n",
    "hidden_size = 128\n",
    "config = PromptBartConfig(image_size = [896,672], image_embedding_size=[28, 21],num_hidden_layers=2,\n",
    "                         num_attention_heads=16,num_key_value_heads=16,intermediate_size=1024,\n",
    "                         hidden_size=hidden_size,BartAttention_implement='flashattn')\n",
    "decoder = PromptBartForCausalLM(config).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec72442-cb5e-4670-82f7-4b73af027606",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder.cuda()\n",
    "encoder_hidden_states  = torch.randn(2, 28*21, hidden_size)\n",
    "input_ids    = torch.randint(0,hidden_size,(2, 10))\n",
    "input_bboxes = torch.randn((2, 10, 4))\n",
    "input_ids[0][5:]=0\n",
    "input_bboxes[0][5:]*=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1675c859-2f79-4719-8644-3cd37f96e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = decoder\n",
    "hidden_states = inputs_embeds  = decoder.decoder.embed_tokens(input_ids.cuda()).cuda()\n",
    "encoder_hidden_states = torch.randn(2, 28*21, hidden_size).cuda()\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "attention_mask[0][5:]=0\n",
    "attention_mask = attention_mask.cuda()\n",
    "B, S, _ = hidden_states.shape\n",
    "B, L, _ = encoder_hidden_states.shape\n",
    "self = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8f170a4-0976-4677-b94f-8e7cef06000c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ed3cdd1-52eb-4288-a561-55f09bf1d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_position = torch.arange(0, 0 + inputs_embeds.shape[1], device=inputs_embeds.device)\n",
    "position_ids = cache_position.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87df06ef-030b-4cd6-b0bc-a603ae3103b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.decoder.locr.modeling_locr import PromptAttentionFlash,PromptAttention,_prepare_4d_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91eec1b6-44df-4504-aa57-e96928de3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6fd358-4f4b-462c-94f5-fae5a1aee94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states = torch.randn(2, 28*21, 1024).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "062975f4-a5ab-42f3-b6f8-ad333c4bc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        decoder.decoder.config.BartAttention_implement = 'flashattn'\n",
    "        causal_mask = decoder.decoder._prepare_decoder_attention_mask(\n",
    "            attention_mask, None, None, input_shape, inputs_embeds,0\n",
    "        )\n",
    "        result1= MBartFlashAttention2.forward(decoder.decoder.layers[0].self_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                attention_mask   = causal_mask.cuda(),   \n",
    "            )[0]\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        decoder.decoder.config.BartAttention_implement = 'eager'\n",
    "        causal_mask = decoder.decoder._prepare_decoder_attention_mask(\n",
    "            attention_mask, None, None, input_shape, inputs_embeds,0\n",
    "        )\n",
    "        result2= MBartAttention.forward(decoder.decoder.layers[0].self_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                attention_mask   = causal_mask.cuda(),   \n",
    "            )[0]\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        decoder.decoder.config.BartAttention_implement = 'sdpa'\n",
    "        causal_mask = decoder.decoder._prepare_decoder_attention_mask(\n",
    "            attention_mask, None, None, input_shape, inputs_embeds,0\n",
    "        )\n",
    "        result3= MBartAttentionSDPA.forward(decoder.decoder.layers[0].self_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                attention_mask   = causal_mask.cuda(),   \n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86ed0ac-a717-4dd3-8175-696c1c22f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = attention_mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f70c8c49-359c-4689-8b97-73d18bfe1ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0200, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(result1[attention_mask],result2[attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d2104f-959e-4ddc-9387-18764e393e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0200, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(result1[attention_mask],result3[attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233ea29b-5030-48ef-9518-c5a50d5fd209",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PromptBartForCausalLM' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m         cross_attn_mask \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n\u001b[1;32m      5\u001b[0m         result1\u001b[38;5;241m=\u001b[39m LlougatCrossAttention\u001b[38;5;241m.\u001b[39mforward(decoder\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcross_attn,\n\u001b[1;32m      6\u001b[0m                 hidden_states    \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mcuda(),              \n\u001b[1;32m      7\u001b[0m                 hidden_states_kv \u001b[38;5;241m=\u001b[39m encoder_hidden_states\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m      8\u001b[0m                 attention_mask   \u001b[38;5;241m=\u001b[39m cross_attn_mask\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      9\u001b[0m             )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PromptBartForCausalLM' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'eager'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result1= LlougatCrossAttention.forward(decoder.model.layers[0].cross_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv = encoder_hidden_states.cuda(),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'flash_attention_2'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result2= LlougatCrossFlashAttention2.forward(decoder.model.layers[0].cross_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv = encoder_hidden_states.cuda(),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'sdpa'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result3= LlougatCrossSdpaAttention.forward(decoder.model.layers[0].cross_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv = encoder_hidden_states.cuda(),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8b14861-8bfc-4ffa-8507-3ee691f21584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(result1[attention_mask],result3[attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5564b3-aea5-4388-b607-0f2239acbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate = torch.randn(2, 28*21, 4).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8993f1be-51e7-40d8-9fbe-6eeba25440ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 μs ± 315 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'eager'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result1= LlougatCrossAttention.forward(decoder.position_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv =(encoder_hidden_states.cuda(), coordinate),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da6ef92-959d-4802-8afd-a39d588d7a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 μs ± 1.34 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        decoder.model.config._attn_implementation = 'sdpa'\n",
    "        cross_attn_mask = decoder.model.get_cross_attn_mask(attention_mask, inputs_embeds,  encoder_hidden_states)\n",
    "        result3= LlougatCrossSdpaAttention.forward(decoder.position_attn,\n",
    "                hidden_states    = hidden_states.cuda(),              \n",
    "                hidden_states_kv =(encoder_hidden_states.cuda(),coordinate ),\n",
    "                attention_mask   = cross_attn_mask.cuda()\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a4a8f2-cfaf-4adf-a25d-091ed71a7ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(result1[attention_mask],result3[attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895376e5-9da2-46b6-befa-5bc7e4253696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The default implementation runs in 666.268 microseconds\n",
      "The math implementation runs in 3759.861 microseconds\n",
      "The flash attention implementation runs in 665.455 microseconds\n",
      "The memory efficient implementation runs in 1816.581 microseconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Example Usage:\n",
    "query, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\n",
    "\n",
    "import torch.utils.benchmark as benchmark\n",
    "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6\n",
    "\n",
    "# Lets define the hyper-parameters of our input\n",
    "batch_size = 32\n",
    "max_sequence_len = 1024\n",
    "num_heads = 32\n",
    "embed_dimension = 32\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "key = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "value = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "\n",
    "print(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
    "\n",
    "# Lets explore the speed of each of the 3 implementations\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "    math_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n",
    "    print(f\"The math implementation runs in {math_time:.3f} microseconds\")\n",
    "\n",
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    try:\n",
    "        flash_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n",
    "        print(f\"The flash attention implementation runs in {flash_time:.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"FlashAttention is not supported. See warnings for reasons.\")\n",
    "\n",
    "with sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):\n",
    "    try:\n",
    "        efficient_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n",
    "        print(f\"The memory efficient implementation runs in {efficient_time:.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"EfficientAttention is not supported. See warnings for reasons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32608354-1ccf-49b6-b926-8148c9939730",
   "metadata": {},
   "source": [
    "# Flash attn native test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af0df696-291b-403b-85a1-7db4136d508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from flash_attn import (\n",
    "    flash_attn_func,\n",
    "    flash_attn_kvpacked_func,\n",
    "    flash_attn_qkvpacked_func,\n",
    "    flash_attn_varlen_func,\n",
    "    flash_attn_varlen_kvpacked_func,\n",
    "    flash_attn_varlen_qkvpacked_func,\n",
    "    flash_attn_with_kvcache,\n",
    ")\n",
    "from flash_attn.bert_padding import pad_input, unpad_input\n",
    "from flash_attn.layers.rotary import apply_rotary_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7ef83f4-5298-4304-9d7d-1bcc1a8d0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_ref(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    query_padding_mask=None,\n",
    "    key_padding_mask=None,\n",
    "    dropout_p=0.0,\n",
    "    dropout_mask=None,\n",
    "    causal=False,\n",
    "    window_size=(-1, -1),  # -1 means infinite window size\n",
    "    upcast=True,\n",
    "    reorder_ops=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        q: (batch_size, seqlen_q, nheads, head_dim)\n",
    "        k: (batch_size, seqlen_k, nheads_k, head_dim)\n",
    "        v: (batch_size, seqlen_k, nheads_k, head_dim)\n",
    "        query_padding_mask: (batch_size, seqlen_q)\n",
    "        key_padding_mask: (batch_size, seqlen_k)\n",
    "        dropout_p: float\n",
    "        dropout_mask: (batch_size, nheads, seqlen_q, seqlen_k)\n",
    "        causal: whether to apply causal masking\n",
    "        window_size: (int, int), left and right window size\n",
    "        upcast: whether to cast all inputs to fp32, do all computation in fp32, then cast\n",
    "            output back to fp16/bf16.\n",
    "        reorder_ops: whether to change the order of operations (scaling k instead of scaling k, etc.)\n",
    "            without changing the math. This is to estimate the numerical error from operation\n",
    "            reordering.\n",
    "    Output:\n",
    "        output: (batch_size, seqlen_q, nheads, head_dim)\n",
    "        attention: (batch_size, nheads, seqlen_q, seqlen_k), softmax after dropout\n",
    "    \"\"\"\n",
    "    if causal:\n",
    "        window_size = (window_size[0], 0)\n",
    "    dtype_og = q.dtype\n",
    "    if upcast:\n",
    "        q, k, v = q.float(), k.float(), v.float()\n",
    "    seqlen_q, seqlen_k = q.shape[1], k.shape[1]\n",
    "    k = repeat(k, \"b s h d -> b s (h g) d\", g=q.shape[2] // k.shape[2])\n",
    "    v = repeat(v, \"b s h d -> b s (h g) d\", g=q.shape[2] // v.shape[2])\n",
    "    d = q.shape[-1]\n",
    "    if not reorder_ops:\n",
    "        scores = torch.einsum(\"bthd,bshd->bhts\", q / math.sqrt(d), k)\n",
    "    else:\n",
    "        scores = torch.einsum(\"bthd,bshd->bhts\", q, k / math.sqrt(d))\n",
    "    if key_padding_mask is not None:\n",
    "        scores.masked_fill_(rearrange(~key_padding_mask, \"b s -> b 1 1 s\"), float(\"-inf\"))\n",
    "    if window_size[0] >= 0 or window_size[1] >= 0:\n",
    "        local_mask = construct_local_mask(\n",
    "            seqlen_q,\n",
    "            seqlen_k,\n",
    "            window_size,\n",
    "            query_padding_mask,\n",
    "            key_padding_mask,\n",
    "            q.device,\n",
    "        )\n",
    "        scores.masked_fill_(local_mask, float(\"-inf\"))\n",
    "    attention = torch.softmax(scores, dim=-1)\n",
    "    # Some rows might be completely masked out so we fill them with zero instead of NaN\n",
    "    if window_size[0] >= 0 or window_size[1] >= 0:\n",
    "        attention = attention.masked_fill(torch.all(local_mask, dim=-1, keepdim=True), 0.0)\n",
    "    # We want to mask here so that the attention matrix doesn't have any NaNs\n",
    "    # Otherwise we'll get NaN in dV\n",
    "    if query_padding_mask is not None:\n",
    "        attention = attention.masked_fill(rearrange(~query_padding_mask, \"b s -> b 1 s 1\"), 0.0)\n",
    "    dropout_scaling = 1.0 / (1 - dropout_p)\n",
    "    # attention_drop = attention.masked_fill(~dropout_mask, 0.0) * dropout_scaling\n",
    "    # output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
    "    if dropout_mask is not None:\n",
    "        attention_drop = attention.masked_fill(~dropout_mask, 0.0)\n",
    "    else:\n",
    "        attention_drop = attention\n",
    "    output = torch.einsum(\"bhts,bshd->bthd\", attention_drop, v * dropout_scaling)\n",
    "    if query_padding_mask is not None:\n",
    "        output.masked_fill_(rearrange(~query_padding_mask, \"b s -> b s 1 1\"), 0.0)\n",
    "    return output.to(dtype=dtype_og), attention.to(dtype=dtype_og)\n",
    "def generate_qkv(\n",
    "    q, k, v, query_padding_mask=None, key_padding_mask=None, kvpacked=False, qkvpacked=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        q: (batch_size, seqlen_q, nheads, d)\n",
    "        k: (batch_size, seqlen_k, nheads_k, d)\n",
    "        v: (batch_size, seqlen_k, nheads_k, d)\n",
    "        query_padding_mask: (batch_size, seqlen), bool\n",
    "        key_padding_mask: (batch_size, seqlen), bool\n",
    "    \"\"\"\n",
    "    assert not (kvpacked and qkvpacked)\n",
    "    batch_size, seqlen_q, nheads, d = q.shape\n",
    "    _, seqlen_k, nheads_k, _ = k.shape\n",
    "    assert k.shape == (batch_size, seqlen_k, nheads_k, d)\n",
    "    assert v.shape == (batch_size, seqlen_k, nheads_k, d)\n",
    "\n",
    "    if query_padding_mask is not None:\n",
    "        q_unpad, indices_q, cu_seqlens_q, max_seqlen_q = unpad_input(q, query_padding_mask)\n",
    "        output_pad_fn = lambda output_unpad: pad_input(output_unpad, indices_q, batch_size, seqlen_q)\n",
    "    else:\n",
    "        q_unpad = rearrange(q, \"b s h d -> (b s) h d\")\n",
    "        cu_seqlens_q = torch.arange(0, (batch_size + 1) * seqlen_q, step=seqlen_q, dtype=torch.int32, device=q_unpad.device)\n",
    "        max_seqlen_q = seqlen_q\n",
    "        output_pad_fn = lambda output_unpad: rearrange(output_unpad, \"(b s) h d -> b s h d\", b=batch_size)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        k_unpad, indices_k, cu_seqlens_k, max_seqlen_k = unpad_input(k, key_padding_mask)\n",
    "        v_unpad, _, _, _ = unpad_input(v, key_padding_mask)\n",
    "    else:\n",
    "        k_unpad = rearrange(k, \"b s h d -> (b s) h d\")\n",
    "        v_unpad = rearrange(v, \"b s h d -> (b s) h d\")\n",
    "        cu_seqlens_k = torch.arange(0, (batch_size + 1) * seqlen_k, step=seqlen_k, dtype=torch.int32, device=k_unpad.device)\n",
    "        max_seqlen_k = seqlen_k\n",
    "\n",
    "    if qkvpacked:\n",
    "        assert (query_padding_mask == key_padding_mask).all()\n",
    "        assert nheads == nheads_k\n",
    "        qkv_unpad = torch.stack([q_unpad, k_unpad, v_unpad], dim=1)\n",
    "        qkv = torch.stack([q, k, v], dim=2)\n",
    "        if query_padding_mask is not None:\n",
    "            dqkv_pad_fn = lambda dqkv_unpad: pad_input(dqkv_unpad, indices_q, batch_size, seqlen_q)\n",
    "        else:\n",
    "            dqkv_pad_fn = lambda dqkv_unpad: rearrange(dqkv_unpad, \"(b s) t h d -> b s t h d\", b=batch_size)\n",
    "        return (\n",
    "            qkv_unpad.detach().requires_grad_(),\n",
    "            cu_seqlens_q,\n",
    "            max_seqlen_q,\n",
    "            qkv.detach().requires_grad_(),\n",
    "            output_pad_fn,\n",
    "            dqkv_pad_fn,\n",
    "        )\n",
    "    elif kvpacked:\n",
    "        kv_unpad = torch.stack([k_unpad, v_unpad], dim=1)\n",
    "        kv = torch.stack([k, v], dim=2)\n",
    "        dq_pad_fn = output_pad_fn\n",
    "        if key_padding_mask is not None:\n",
    "            dkv_pad_fn = lambda dkv_unpad: pad_input(dkv_unpad, indices_k, batch_size, seqlen_k)\n",
    "        else:\n",
    "            dkv_pad_fn = lambda dkv_unpad: rearrange(\n",
    "                dkv_unpad, \"(b s) t h d -> b s t h d\", b=batch_size\n",
    "            )\n",
    "        return (\n",
    "            q_unpad.detach().requires_grad_(),\n",
    "            kv_unpad.detach().requires_grad_(),\n",
    "            cu_seqlens_q,\n",
    "            cu_seqlens_k,\n",
    "            max_seqlen_q,\n",
    "            max_seqlen_k,\n",
    "            q.detach().requires_grad_(),\n",
    "            kv.detach().requires_grad_(),\n",
    "            output_pad_fn,\n",
    "            dq_pad_fn,\n",
    "            dkv_pad_fn,\n",
    "        )\n",
    "    else:\n",
    "        dq_pad_fn = output_pad_fn\n",
    "        if key_padding_mask is not None:\n",
    "            dk_pad_fn = lambda dk_unpad: pad_input(dk_unpad, indices_k, batch_size, seqlen_k)\n",
    "        else:\n",
    "            dk_pad_fn = lambda dk_unpad: rearrange(dk_unpad, \"(b s) h d -> b s h d\", b=batch_size)\n",
    "        return (\n",
    "            q_unpad.detach().requires_grad_(),\n",
    "            k_unpad.detach().requires_grad_(),\n",
    "            v_unpad.detach().requires_grad_(),\n",
    "            cu_seqlens_q,\n",
    "            cu_seqlens_k,\n",
    "            max_seqlen_q,\n",
    "            max_seqlen_k,\n",
    "            q.detach().requires_grad_(),\n",
    "            k.detach().requires_grad_(),\n",
    "            v.detach().requires_grad_(),\n",
    "            output_pad_fn,\n",
    "            dq_pad_fn,\n",
    "            dk_pad_fn,\n",
    "        )\n",
    "def generate_random_padding_mask(max_seqlen, batch_size, device, mode=\"random\"):\n",
    "    assert mode in [\"full\", \"random\", \"third\"]\n",
    "    if mode == \"full\":\n",
    "        lengths = torch.full((batch_size, 1), max_seqlen, device=device, dtype=torch.int32)\n",
    "    elif mode == \"random\":\n",
    "        lengths = torch.randint(\n",
    "            max(1, max_seqlen - 20), max_seqlen + 1, (batch_size, 1), device=device\n",
    "        )\n",
    "    elif mode == \"third\":\n",
    "        lengths = torch.randint(max_seqlen // 3, max_seqlen + 1, (batch_size, 1), device=device)\n",
    "    padding_mask = (\n",
    "        repeat(torch.arange(max_seqlen, device=device), \"s -> b s\", b=batch_size) < lengths\n",
    "    )\n",
    "    return padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bae3cdcc-5752-421f-8db6-8b75be1206cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _upad_input(query_layer, key_layer, value_layer, attention_mask, query_length):\n",
    "        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n",
    "        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n",
    "        \n",
    "        key_layer = index_first_axis(\n",
    "            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n",
    "        )\n",
    "        value_layer = index_first_axis(\n",
    "            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n",
    "        )\n",
    "        if query_length == kv_seq_len:\n",
    "            num_heads = query_layer.shape[-2]\n",
    "            query_layer = index_first_axis(\n",
    "                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n",
    "            )\n",
    "            cu_seqlens_q = cu_seqlens_k\n",
    "            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n",
    "            indices_q = indices_k\n",
    "        elif query_length == 1:\n",
    "            max_seqlen_in_batch_q = 1\n",
    "            cu_seqlens_q = torch.arange(\n",
    "                batch_size + 1, dtype=torch.int32, device=query_layer.device\n",
    "            )  # There is a memcpy here, that is very bad.\n",
    "            indices_q = cu_seqlens_q[:-1]\n",
    "            query_layer = query_layer.squeeze(1)\n",
    "        else:\n",
    "            # The -q_len: slice assumes left padding.\n",
    "            attention_mask = attention_mask[:, -query_length:]\n",
    "            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n",
    "\n",
    "        return (\n",
    "            query_layer,\n",
    "            key_layer,\n",
    "            value_layer,\n",
    "            indices_q,\n",
    "            (cu_seqlens_q, cu_seqlens_k),\n",
    "            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e256ddcc-fe13-466b-a2f0-76cd50dfe859",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ref, attn_ref = attention_ref(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            query_padding_mask,\n",
    "            key_padding_mask,\n",
    "            dropout_p,\n",
    "            dropout_mask,\n",
    "            causal=causal,\n",
    "            window_size=window_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0ad05c66-d680-4186-adae-3a080bdc6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seqlen_q = 10\n",
    "seqlen_k = 213\n",
    "nheads = nheads_k = 16 \n",
    "d = 32\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "q = torch.randn(batch_size, seqlen_q,   nheads, d, device=device, dtype=dtype, requires_grad=True)\n",
    "k = torch.randn(batch_size, seqlen_k, nheads_k, d, device=device, dtype=dtype, requires_grad=True)\n",
    "v = torch.randn(batch_size, seqlen_k, nheads_k, d, device=device, dtype=dtype, requires_grad=True)\n",
    "query_padding_mask = generate_random_padding_mask(seqlen_q, batch_size, device, mode=\"random\")\n",
    "key_padding_mask = None\n",
    "dropout_p = 0\n",
    "causal = False\n",
    "window_size=(-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7b34e753-2e63-4dd9-b4ce-3e22e608fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LLAMA_ATTENTION_CLASSES, LlamaAttention, LlamaMLP, AttentionMaskConverter, Cache, LlamaRMSNorm, StaticCache, apply_rotary_pos_emb, repeat_kv, is_flash_attn_2_available, _get_unpad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4b0a0974-9353-46a8-a8c6-7ad83c4e8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_layer,key_layer,value_layer,indices_q,(cu_seqlens_q, cu_seqlens_k),(max_seqlen_in_batch_q, max_seqlen_in_batch_k) =  _upad_input(q, k, v, query_padding_mask, seqlen_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "76cceebe-6286-4023-b02a-f194f780dfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 213, 16, 32])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b313ecb4-1e5d-41a3-bca6-d80b73e423db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 16, 32])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ff8048e5-0ca3-462a-9754-604863d7b4b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (9) must match the size of tensor b (426) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk_unpad\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9) must match the size of tensor b (426) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "torch.dist(key_layer,k_unpad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c4f03509-438b-48d9-8c17-7adfc0ee8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    q_unpad,\n",
    "    k_unpad,\n",
    "    v_unpad,\n",
    "    cu_seqlens_q,\n",
    "    cu_seqlens_k,\n",
    "    max_seqlen_q,\n",
    "    max_seqlen_k,\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    output_pad_fn,\n",
    "    dq_pad_fn,\n",
    "    dk_pad_fn,\n",
    ") = generate_qkv(q, k, v, query_padding_mask, key_padding_mask, kvpacked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89e229-2b79-4ff1-9c86-ff7ce87e1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_unpad, sm_lse, S_dmask = flash_attn_varlen_func(\n",
    "    q_unpad,\n",
    "    k_unpad,\n",
    "    v_unpad,\n",
    "    cu_seqlens_q,\n",
    "    cu_seqlens_k,\n",
    "    max_seqlen_q,\n",
    "    max_seqlen_k,\n",
    "    dropout_p,\n",
    "    return_attn_probs=True,\n",
    "    causal=causal,\n",
    "    window_size=window_size,\n",
    ")\n",
    "out = output_pad_fn(out_unpad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4b24c723-428a-4f8f-a954-89998cf4bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ref, attn_ref = attention_ref(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    query_padding_mask,\n",
    "    key_padding_mask,\n",
    "    0.0,\n",
    "    None,\n",
    "    causal=causal,\n",
    "    window_size=window_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "123452b3-0849-4020-93dd-a01734737cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 16, 32])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a7ccacb0-a577-4f6c-bb39-8d68ddd2094c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 213, 16, 32])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "773479d3-e2e6-4ef7-b230-f871ff43cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.model.config._attn_implementation = 'eager'\n",
    "attention_mask = decoder.model.get_cross_attn_mask(query_padding_mask, query_states,  key_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6379d9d8-3257-414e-8ff4-84dbfeb16d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10, 213])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2e02a43e-499b-475b-8f91-d1726246c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = q\n",
    "key_states   = k \n",
    "value_states = v\n",
    "attn_weights = torch.einsum(\"bthd,bshd->bhts\", query_states / math.sqrt(d), key_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1d8f685a-af15-4e33-b51c-090972a94910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10, 16])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask  = attention_mask[:, :, :, : key_states.shape[-2]] ##\n",
    "causal_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c451a005-ea09-4c1d-920e-4442bc2e335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if attention_mask is not None:  # no matter the length, we just slice it\n",
    "    causal_mask  = attention_mask[:, :, :, : key_states.shape[1]] ##\n",
    "    attn_weights = attn_weights + causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0e5d2e44-fa34-45f5-ba75-3193bb363724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upcast attention to fp32\n",
    "attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "attn_output  = torch.einsum(\"bhts,bshd->bthd\", attn_weights, value_states)\n",
    "bsz, q_len, _, _ = query_states.shape\n",
    "bsz, v_len, H, d = value_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cc820705-2566-4300-a9c0-8f78badf3701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 16, 32])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "241d24b2-9b09-4494-8eaf-b7e2e15833b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0047, device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(attn_output[query_padding_mask],out_ref[query_padding_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2e7182ef-3779-4ea0-adf3-16f265d3a04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0021, device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(out,out_ref)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unarxive",
   "language": "python",
   "name": "unarxive"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
